{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f02a0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================================>                   (26 + 14) / 40]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Starts a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Taxi vs Rideshare Profitability\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", False)   \n",
    "        .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "        .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"64\")        \n",
    "        .config(\"spark.driver.memory\", \"6g\")                \n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Define months\n",
    "months = [\"2024-01\",\"2024-02\",\"2024-03\",\"2024-04\",\"2024-05\",\"2024-06\"]\n",
    "\n",
    "# Load in data files \n",
    "yellow_files = [f\"data/yellow/yellow_tripdata_{m}.parquet\" for m in months]\n",
    "fhvhv_files  = [f\"data/fhvhv/fhvhv_tripdata_{m}.parquet\"   for m in months]\n",
    "\n",
    "df_yellow = (\n",
    "    spark.read.parquet(*yellow_files)\n",
    "         .withColumn(\"service_type\", lit(\"yellow\"))\n",
    ")\n",
    "df_fhvhv = (\n",
    "    spark.read.parquet(*fhvhv_files)\n",
    "         .withColumn(\"service_type\", lit(\"hv_fhv\"))\n",
    ")\n",
    "\n",
    "# Merge\n",
    "df = df_yellow.unionByName(df_fhvhv, allowMissingColumns=True)\n",
    "\n",
    "# External tables (you'll wire these in later for fuel/energy)\n",
    "electricity = spark.read.csv(\"data/external/electricity.csv\", header=True, inferSchema=True)\n",
    "fuel        = spark.read.csv(\"data/external/fuel.csv\", header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c04cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================>                  (27 + 13) / 40]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=======================================================> (39 + 1) / 40]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess data\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, coalesce, unix_timestamp, when, lit, date_format, hour, dayofweek, broadcast\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Reduce memory usage\n",
    "_needed = [\n",
    "    \"service_type\",\n",
    "    \"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\n",
    "    \"pickup_datetime\",\"dropoff_datetime\",\n",
    "    \"trip_distance\",\"trip_miles\",\"trip_time\",\n",
    "    \"PULocationID\",\"DOLocationID\",\n",
    "    \"passenger_count\",\"payment_type\",\n",
    "    \"fare_amount\",\"extra\",\"tip_amount\",\n",
    "    \"driver_pay\",\"tips\"\n",
    "]\n",
    "df = df.select([c for c in _needed if c in df.columns])\n",
    "\n",
    "# Standardise timestamps\n",
    "df = (\n",
    "    df.withColumn(\"pickup_ts\",  to_timestamp(coalesce(col(\"tpep_pickup_datetime\"),  col(\"pickup_datetime\"))))\n",
    "      .withColumn(\"dropoff_ts\", to_timestamp(coalesce(col(\"tpep_dropoff_datetime\"), col(\"dropoff_datetime\"))))\n",
    ")\n",
    "\n",
    "# Remove rows with null pickup or dropoff timestamps\n",
    "df = df.filter(col(\"pickup_ts\").isNotNull() & col(\"dropoff_ts\").isNotNull())\n",
    "\n",
    "# Standardise location IDs\n",
    "df = df.withColumn(\"month\", date_format(col(\"pickup_ts\"), \"yyyy-MM\"))\n",
    "\n",
    "# Standardise distance and keep positive distances only and not null\n",
    "df = (\n",
    "    df.withColumn(\"distance_mi\", coalesce(col(\"trip_distance\"), col(\"trip_miles\")).cast(DoubleType()))\n",
    "      .filter(col(\"distance_mi\").isNotNull() & (col(\"distance_mi\") > 0))\n",
    ")\n",
    "\n",
    "# Standardise trip time and not null\n",
    "df = df.withColumn(\n",
    "    \"trip_time_s\",\n",
    "    when(col(\"trip_time\").isNotNull(), col(\"trip_time\").cast(\"double\"))\n",
    "    .otherwise((unix_timestamp(col(\"dropoff_ts\")) - unix_timestamp(col(\"pickup_ts\"))).cast(\"double\"))\n",
    ")\n",
    "df = df.filter(col(\"trip_time_s\").isNotNull() & (col(\"trip_time_s\") > 0))\n",
    "\n",
    "# Deduplicate rows \n",
    "dedupe_key = [c for c in [\"service_type\",\"pickup_ts\",\"dropoff_ts\",\"PULocationID\",\"DOLocationID\",\"distance_mi\",\"trip_time_s\"] if c in df.columns]\n",
    "\n",
    "# If dedupe_key is empty, we won't deduplicate\n",
    "if dedupe_key:\n",
    "    w = Window.partitionBy([\"month\"] + dedupe_key).orderBy(F.lit(1))\n",
    "    df = df.withColumn(\"__rn\", F.row_number().over(w)).filter(col(\"__rn\") == 1).drop(\"__rn\")\n",
    "\n",
    "# Fixed Parameters\n",
    "CREDIT_CARD_FEE = 0.025 \n",
    "MAINTENANCE_COST_PER_MILE = 0.15\n",
    "MAINTENANCE_COST_PER_MILE_HV = 0.15\n",
    "\n",
    "# Payment type exists\n",
    "if \"payment_type\" not in df.columns:\n",
    "    df = df.withColumn(\"payment_type\", lit(None).cast(IntegerType()))\n",
    "\n",
    "# Time features \n",
    "df = (df\n",
    "    .withColumn(\"pickup_hour\", hour(col(\"pickup_ts\")))\n",
    "    .withColumn(\"pickup_dow\", dayofweek(col(\"pickup_ts\")))  \n",
    "    .withColumn(\"is_weekend\", (col(\"pickup_dow\").isin([1,7])).cast(\"boolean\"))\n",
    ")\n",
    "\n",
    "# Add revenue \n",
    "rev_yellow = coalesce(col(\"fare_amount\"), lit(0.0)) + coalesce(col(\"extra\"), lit(0.0)) + coalesce(col(\"tip_amount\"), lit(0.0))\n",
    "rev_hv     = coalesce(col(\"driver_pay\"), lit(0.0)) + coalesce(col(\"tips\"), lit(0.0))\n",
    "df = df.withColumn(\n",
    "    \"revenue\",\n",
    "    when(col(\"service_type\") == \"yellow\", rev_yellow).otherwise(rev_hv).cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Add costs\n",
    "maint_rate = when(col(\"service_type\") == \"yellow\",\n",
    "                  lit(MAINTENANCE_COST_PER_MILE)\n",
    "              ).otherwise(\n",
    "                  lit(MAINTENANCE_COST_PER_MILE_HV)\n",
    "              )\n",
    "df = df.withColumn(\"expense_maintenance\", (col(\"distance_mi\") * maint_rate).cast(DoubleType()))\n",
    "\n",
    "# Credit card fee \n",
    "df = df.withColumn(\n",
    "     \"expense_cc_processing\",\n",
    "    when((col(\"service_type\") == \"yellow\") & (col(\"payment_type\") == 1),\n",
    "         (lit(CREDIT_CARD_FEE) * col(\"revenue\")).cast(DoubleType()))\n",
    "    .otherwise(lit(0.0))\n",
    ")\n",
    "\n",
    "# Expenses pre-fuel \n",
    "df = df.withColumn(\n",
    "    \"expenses_nonfuel\",\n",
    "    (col(\"expense_maintenance\") + col(\"expense_cc_processing\")).cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Keep distance\n",
    "df = df.filter(col(\"distance_mi\") >= 0.1)\n",
    "\n",
    "# Make sure within month range\n",
    "df = df.filter( (col(\"month\") >= \"2024-01\") & (col(\"month\") <= \"2024-06\") )\n",
    "\n",
    "# Keep duration >= 60 seconds\n",
    "df = df.filter(col(\"trip_time_s\") >= 60)\n",
    "\n",
    "# Keep positive passenger count \n",
    "if \"passenger_count\" in df.columns:\n",
    "    df = df.filter(\n",
    "        when(col(\"service_type\") == \"yellow\", col(\"passenger_count\") > 0)\n",
    "        .otherwise(True)\n",
    "    )\n",
    "\n",
    "# Valid TLC zone IDs \n",
    "for c in [\"PULocationID\", \"DOLocationID\"]:\n",
    "    if c in df.columns:\n",
    "        df = df.filter((col(c) >= 1) & (col(c) <= 263))\n",
    "\n",
    "# Non-negative money fields \n",
    "money_ok = (\n",
    "    (coalesce(col(\"fare_amount\"), lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"extra\"),       lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"tip_amount\"),  lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"driver_pay\"),  lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"tips\"),        lit(0.0))  >= 0)\n",
    ")\n",
    "df = df.filter(money_ok)\n",
    "\n",
    "# Minimum initial fare for Yellow \n",
    "df = df.filter(\n",
    "    when(col(\"service_type\") == \"yellow\", coalesce(col(\"fare_amount\"), lit(0.0)) >= 1.50)\n",
    "    .otherwise(True)\n",
    ")\n",
    "\n",
    "\n",
    "# Pre-fuel profitability\n",
    "df = (df\n",
    "    .withColumn(\"active_hours\", (col(\"trip_time_s\") / 3600.0).cast(DoubleType()))\n",
    "    .withColumn(\"net_before_fuel\", (col(\"revenue\") - col(\"expenses_nonfuel\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_per_hr_before_fuel\", (col(\"net_before_fuel\") / col(\"active_hours\")).cast(DoubleType()))\n",
    "    .withColumn(\"mph\", (col(\"distance_mi\") / (col(\"trip_time_s\")/3600.0)).cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Single-pass outlier trim using 99.9% quantile\n",
    "stacked = (\n",
    "    df.select(\"service_type\", F.lit(\"distance_mi\").alias(\"metric\"), col(\"distance_mi\").alias(\"value\"))\n",
    "      .unionByName(df.select(\"service_type\", F.lit(\"trip_time_s\").alias(\"metric\"), col(\"trip_time_s\").alias(\"value\")))\n",
    "      .unionByName(df.select(\"service_type\", F.lit(\"revenue\").alias(\"metric\"),     col(\"revenue\").alias(\"value\")))\n",
    ")\n",
    "bounds = (\n",
    "    stacked.groupBy(\"service_type\", \"metric\")\n",
    "           .agg(F.expr(\"percentile_approx(value, 0.999, 10000)\").alias(\"p999\"))\n",
    ")\n",
    "df = (\n",
    "    df.alias(\"t\")\n",
    "      .join(bounds.alias(\"b1\").filter(col(\"b1.metric\") == \"distance_mi\")\n",
    "                 .select(col(\"service_type\").alias(\"s1\"), col(\"p999\").alias(\"p_d\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s1\")], how=\"left\")\n",
    "      .join(bounds.alias(\"b2\").filter(col(\"b2.metric\") == \"trip_time_s\")\n",
    "                 .select(col(\"service_type\").alias(\"s2\"), col(\"p999\").alias(\"p_t\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s2\")], how=\"left\")\n",
    "      .join(bounds.alias(\"b3\").filter(col(\"b3.metric\") == \"revenue\")\n",
    "                 .select(col(\"service_type\").alias(\"s3\"), col(\"p999\").alias(\"p_r\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s3\")], how=\"left\")\n",
    "      .filter( (col(\"distance_mi\") <= F.coalesce(col(\"p_d\"), lit(float(\"inf\")))) &\n",
    "               (col(\"trip_time_s\") <= F.coalesce(col(\"p_t\"), lit(float(\"inf\")))) &\n",
    "               (col(\"revenue\")     <= F.coalesce(col(\"p_r\"), lit(float(\"inf\")))) )\n",
    "      .drop(\"s1\",\"s2\",\"s3\",\"p_d\",\"p_t\",\"p_r\")\n",
    ")\n",
    "\n",
    "df = df.repartition(64, \"service_type\", \"month\")\n",
    "\n",
    "# Cap impossible speeds\n",
    "df = df.filter((col(\"mph\") >= 0) & (col(\"mph\") <= 120.0))\n",
    "\n",
    "\n",
    "# Fuel and energy costs\n",
    "fuel = fuel.select(\"month\", \"price_per_gallon\").dropna()\n",
    "electricity = electricity.select(\"month\", \"price_usd_per_kwh\").dropna()\n",
    "\n",
    "# Join fuel and electricity prices\n",
    "df = (df.join(broadcast(fuel), on=\"month\", how=\"left\")\n",
    "        .join(broadcast(electricity), on=\"month\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Energy assumptions according to EPA and AFDC \n",
    "MPG_FHV  = 27.0  \n",
    "MPG_TAXI = 16.0  \n",
    "\n",
    "KWH_YELLOW = 0.30\n",
    "KWH_FHV    = 0.30\n",
    "\n",
    "YELLOW_EV_PERCENT = 0.00  # Assuming no EVs in Yellow Taxi fleet \n",
    "FHV_EV_PERCENT    = 0.10  # Example share for HVFHV\n",
    "\n",
    "# Per-service parameters as columns \n",
    "df = (df\n",
    "    .withColumn(\n",
    "        \"mpg\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(MPG_TAXI))\n",
    "        .otherwise(lit(MPG_FHV)).cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"kwh_per_mile\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(KWH_YELLOW))\n",
    "        .otherwise(lit(KWH_FHV)).cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ev_share\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(YELLOW_EV_PERCENT))\n",
    "        .otherwise(lit(FHV_EV_PERCENT)).cast(DoubleType())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cost per mile (blend gas vs EV by ev_share)\n",
    "gas_cpm = (col(\"price_per_gallon\") / col(\"mpg\")).cast(DoubleType())\n",
    "ev_cpm  = (col(\"price_usd_per_kwh\") * col(\"kwh_per_mile\")).cast(DoubleType())\n",
    "\n",
    "df = (df\n",
    "    .withColumn(\n",
    "        \"energy_cost_per_mile\",\n",
    "        ((lit(1.0) - coalesce(col(\"ev_share\"), lit(0.0))) * coalesce(gas_cpm, lit(0.0))) +\n",
    "        (coalesce(col(\"ev_share\"), lit(0.0)) * coalesce(ev_cpm, lit(0.0)))\n",
    "    )\n",
    "    .withColumn(\"expense_fuel\", (col(\"distance_mi\") * col(\"energy_cost_per_mile\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_after_fuel\", (col(\"revenue\") - col(\"expenses_nonfuel\") - col(\"expense_fuel\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_per_hr_after_fuel\", (col(\"net_after_fuel\") / col(\"active_hours\")).cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Keep only relevant columns\n",
    "cols_keep = [\n",
    "    # Metadata\n",
    "    \"service_type\", \"month\", \"pickup_ts\", \"dropoff_ts\",\n",
    "    \"pickup_hour\", \"pickup_dow\", \"is_weekend\",\n",
    "    # Location IDs\n",
    "    \"PULocationID\", \"DOLocationID\",\n",
    "    # engineered trip metrics\n",
    "    \"distance_mi\", \"trip_time_s\", \"mph\", \"active_hours\",\n",
    "    # Feature engineering\n",
    "    \"revenue\", \"expenses_nonfuel\", \"expense_fuel\",\n",
    "    \"net_before_fuel\", \"net_after_fuel\",\n",
    "    \"net_per_hr_before_fuel\", \"net_per_hr_after_fuel\",\n",
    "    # Parameters\n",
    "    \"price_per_gal\", \"price_usd_per_kwh\",\n",
    "    \"energy_cost_per_mile\", \"ev_share\", \"mpg\", \"kwh_per_mile\",\n",
    "]\n",
    "\n",
    "# Filter columns to keep only those that exist in the DataFrame\n",
    "cols_keep = [c for c in cols_keep if c in df.columns]\n",
    "df = df.select(*cols_keep)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17467e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=========>     (25 + 15) / 40][Stage 19:>                (0 + 1) / 40]\r"
     ]
    }
   ],
   "source": [
    "# Analyze results and Geospatial Analysis\n",
    "# ===============================\n",
    "# ANALYSIS + GEOSPATIAL PREP\n",
    "# ===============================\n",
    "from pyspark.sql.functions import col, sum as ssum, count, round as sround, when, lit, expr\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---- speed toggle for dev runs ----\n",
    "FAST_MODE = False  # True = ~2% stratified sample, False = full data\n",
    "if FAST_MODE:\n",
    "    df_base = df.stat.sampleBy(\"service_type\", {\"yellow\":0.02, \"hv_fhv\":0.02}, seed=7)\n",
    "else:\n",
    "    df_base = df\n",
    "\n",
    "# ---------- 0) small helpers (no UDFs) ----------\n",
    "# time-weighted aggregator fragment used below\n",
    "sum_net   = ssum(\"net_after_fuel\").alias(\"sum_net\")\n",
    "sum_hours = ssum(\"active_hours\").alias(\"sum_hours\")\n",
    "\n",
    "# ---------- 1) overall time-weighted net $/hr ----------\n",
    "overall = (\n",
    "    df_base.groupBy(\"service_type\")\n",
    "           .agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    "           .withColumn(\"net_per_hr_TW\", sround(col(\"sum_net\")/col(\"sum_hours\"), 2))\n",
    ")\n",
    "\n",
    "# ---------- 2) monthly time-weighted net $/hr ----------\n",
    "monthly = (\n",
    "    df_base.groupBy(\"service_type\",\"month\")\n",
    "           .agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    "           .withColumn(\"net_per_hr_TW\", sround(col(\"sum_net\")/col(\"sum_hours\"), 2))\n",
    "           .orderBy(\"service_type\",\"month\")\n",
    ")\n",
    "\n",
    "# ---------- 3) hour-of-day profile ----------\n",
    "hod = (\n",
    "    df_base.groupBy(\"service_type\",\"pickup_hour\")\n",
    "           .agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    "           .withColumn(\"net_per_hr_TW\", sround(col(\"sum_net\")/col(\"sum_hours\"), 2))\n",
    "           .orderBy(\"service_type\",\"pickup_hour\")\n",
    ")\n",
    "\n",
    "# ---------- 4) weekday vs weekend ----------\n",
    "wkend = (\n",
    "    df_base.groupBy(\"service_type\",\"is_weekend\")\n",
    "           .agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    "           .withColumn(\"net_per_hr_TW\", sround(col(\"sum_net\")/col(\"sum_hours\"), 2))\n",
    "           .withColumn(\"week_type\", when(col(\"is_weekend\"), lit(\"weekend\")).otherwise(lit(\"weekday\")))\n",
    "           .select(\"service_type\",\"week_type\",\"trips\",\"sum_net\",\"sum_hours\",\"net_per_hr_TW\")\n",
    "           .orderBy(\"service_type\",\"week_type\")\n",
    ")\n",
    "\n",
    "# ---------- 5) earnings composition (per trip averages) ----------\n",
    "comp = (\n",
    "    df_base.groupBy(\"service_type\")\n",
    "           .agg(\n",
    "               ssum(\"revenue\").alias(\"sum_rev\"),\n",
    "               ssum(\"expenses_nonfuel\").alias(\"sum_nonfuel\"),\n",
    "               ssum(\"expense_fuel\").alias(\"sum_fuel\"),\n",
    "               count(\"*\").alias(\"n_trips\")\n",
    "           )\n",
    "           .withColumn(\"rev_per_trip\",     sround(col(\"sum_rev\")/col(\"n_trips\"), 2))\n",
    "           .withColumn(\"nonfuel_per_trip\", sround(col(\"sum_nonfuel\")/col(\"n_trips\"), 2))\n",
    "           .withColumn(\"fuel_per_trip\",    sround(col(\"sum_fuel\")/col(\"n_trips\"), 2))\n",
    "           .select(\"service_type\",\"n_trips\",\"rev_per_trip\",\"nonfuel_per_trip\",\"fuel_per_trip\")\n",
    ")\n",
    "\n",
    "# ---------- 6) efficiency: revenue & net per mile (quantiles) ----------\n",
    "eff = (df_base\n",
    "       .withColumn(\"rev_per_mi\", col(\"revenue\")/col(\"distance_mi\"))\n",
    "       .withColumn(\"net_per_mi\", col(\"net_after_fuel\")/col(\"distance_mi\"))\n",
    "       .filter(col(\"distance_mi\") > 0.1)\n",
    ")\n",
    "\n",
    "eff_q = (\n",
    "    eff.groupBy(\"service_type\")\n",
    "       .agg(\n",
    "           F.percentile_approx(\"rev_per_mi\", [0.25,0.5,0.75], 1000).alias(\"rev_q\"),\n",
    "           F.percentile_approx(\"net_per_mi\", [0.25,0.5,0.75], 1000).alias(\"net_q\")\n",
    "       )\n",
    "       .select(\n",
    "           \"service_type\",\n",
    "           col(\"rev_q\")[0].alias(\"rev_per_mi_q25\"),\n",
    "           col(\"rev_q\")[1].alias(\"rev_per_mi_q50\"),\n",
    "           col(\"rev_q\")[2].alias(\"rev_per_mi_q75\"),\n",
    "           col(\"net_q\")[0].alias(\"net_per_mi_q25\"),\n",
    "           col(\"net_q\")[1].alias(\"net_per_mi_q50\"),\n",
    "           col(\"net_q\")[2].alias(\"net_per_mi_q75\")\n",
    "       )\n",
    ")\n",
    "\n",
    "# ---------- 7) pickup zone: time-weighted net $/hr + trips (choropleth) ----------\n",
    "zone = (\n",
    "    df_base.groupBy(\"service_type\",\"PULocationID\")\n",
    "           .agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    "           .withColumn(\"net_per_hr_TW\", sround(col(\"sum_net\")/col(\"sum_hours\"), 2))\n",
    ")\n",
    "zone_y  = zone.filter(col(\"service_type\")==\"yellow\")\n",
    "zone_hv = zone.filter(col(\"service_type\")==\"hv_fhv\")\n",
    "\n",
    "# ---------- 8) difference map: Yellow minus HVFHV net/hr by pickup zone ----------\n",
    "zone_pivot = (\n",
    "    zone.groupBy(\"PULocationID\")\n",
    "        .pivot(\"service_type\", [\"yellow\",\"hv_fhv\"])\n",
    "        .agg(F.first(\"net_per_hr_TW\"))\n",
    "        .withColumn(\"diff_yellow_minus_hv\", sround(col(\"yellow\") - col(\"hv_fhv\"), 2))\n",
    ")\n",
    "\n",
    "# ---------- 9) pickup density map ----------\n",
    "zone_trips = (\n",
    "    df_base.groupBy(\"service_type\",\"PULocationID\")\n",
    "           .agg(count(\"*\").alias(\"trips\"))\n",
    "           .orderBy(\"service_type\", col(\"trips\").desc())\n",
    ")\n",
    "\n",
    "# ---------- 10) top OD flows (for desire-lines) ----------\n",
    "flows = (\n",
    "    df_base.groupBy(\"service_type\",\"PULocationID\",\"DOLocationID\")\n",
    "           .count().withColumnRenamed(\"count\",\"trips\")\n",
    ")\n",
    "flows_y_top  = flows.filter(col(\"service_type\")==\"yellow\").orderBy(col(\"trips\").desc()).limit(200)\n",
    "flows_hv_top = flows.filter(col(\"service_type\")==\"hv_fhv\").orderBy(col(\"trips\").desc()).limit(200)\n",
    "\n",
    "# ---------- 11) airport corridors ----------\n",
    "# (check your taxi_zones lookup if you want to confirm these IDs)\n",
    "JFK_IDS = [132]   # JFK Airport\n",
    "LGA_IDS = [138]   # LaGuardia Airport\n",
    "# EWR Newark is LocationID 1 in TLC zones (optional): EWR_IDS = [1]\n",
    "\n",
    "airport_pickups = (\n",
    "    df_base.withColumn(\n",
    "        \"airport\",\n",
    "        when(col(\"PULocationID\").isin(JFK_IDS), lit(\"JFK\"))\n",
    "        .when(col(\"PULocationID\").isin(LGA_IDS), lit(\"LGA\"))\n",
    "        .otherwise(lit(\"OTHER\"))\n",
    "    )\n",
    "    .groupBy(\"service_type\",\"airport\")\n",
    "    .agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    "    .withColumn(\"net_per_hr_TW\", sround(col(\"sum_net\")/col(\"sum_hours\"), 2))\n",
    "    .orderBy(\"service_type\",\"airport\")\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# WRITE ALL CSVs (pick what you need)\n",
    "# ===============================\n",
    "overall.coalesce(1).write.mode(\"overwrite\").csv(\"plots/overall.csv\", header=True)\n",
    "monthly.coalesce(1).write.mode(\"overwrite\").csv(\"plots/monthly.csv\", header=True)\n",
    "hod.coalesce(1).write.mode(\"overwrite\").csv(\"plots/hour_of_day.csv\", header=True)\n",
    "wkend.coalesce(1).write.mode(\"overwrite\").csv(\"plots/weekday_weekend.csv\", header=True)\n",
    "comp.coalesce(1).write.mode(\"overwrite\").csv(\"plots/composition.csv\", header=True)\n",
    "eff_q.coalesce(1).write.mode(\"overwrite\").csv(\"plots/efficiency_quantiles.csv\", header=True)\n",
    "\n",
    "zone_y.coalesce(1).write.mode(\"overwrite\").csv(\"plots/zone_yellow.csv\", header=True)\n",
    "zone_hv.coalesce(1).write.mode(\"overwrite\").csv(\"plots/zone_hvfhv.csv\", header=True)\n",
    "zone_pivot.coalesce(1).write.mode(\"overwrite\").csv(\"plots/zone_diff_y_minus_hv.csv\", header=True)\n",
    "zone_trips.coalesce(1).write.mode(\"overwrite\").csv(\"plots/zone_trips.csv\", header=True)\n",
    "\n",
    "flows_y_top.coalesce(1).write.mode(\"overwrite\").csv(\"plots/flows_top200_yellow.csv\", header=True)\n",
    "flows_hv_top.coalesce(1).write.mode(\"overwrite\").csv(\"plots/flows_top200_hvfhv.csv\", header=True)\n",
    "\n",
    "airport_pickups.coalesce(1).write.mode(\"overwrite\").csv(\"plots/airport_pickups.csv\", header=True)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PLOTTING: analysis + geospatial (matplotlib / GeoPandas)\n",
    "# ==========================================\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def load_csv(dirpath):\n",
    "    \"\"\"Load a Spark-written CSV folder (with part-*.csv) into a pandas DF.\"\"\"\n",
    "    paths = sorted(glob.glob(os.path.join(dirpath, \"*.csv\")))\n",
    "    if not paths:\n",
    "        paths = sorted(glob.glob(os.path.join(dirpath, \"part-*.csv\")))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {dirpath}\")\n",
    "    dfs = [pd.read_csv(p) for p in paths]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "os.makedirs(\"plots/img\", exist_ok=True)\n",
    "\n",
    "# ---------- 1) Overall (table print) ----------\n",
    "overall = load_csv(\"plots/overall.csv\")\n",
    "print(\"\\nOVERALL (time-weighted net/hr):\")\n",
    "print(overall)\n",
    "\n",
    "# ---------- 2) Monthly net/hr (line chart) ----------\n",
    "monthly = load_csv(\"plots/monthly.csv\")\n",
    "monthly_p = monthly.pivot(index=\"month\", columns=\"service_type\", values=\"net_per_hr_TW\").sort_index()\n",
    "ax = monthly_p.plot(marker=\"o\")\n",
    "ax.set_title(\"Time-weighted Net $/hr by Month\")\n",
    "ax.set_ylabel(\"Net $/hr\")\n",
    "ax.set_xlabel(\"Month (2024)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/monthly_net_per_hr.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 3) Hour-of-day profile ----------\n",
    "hod = load_csv(\"plots/hour_of_day.csv\")\n",
    "hod_p = hod.pivot(index=\"pickup_hour\", columns=\"service_type\", values=\"net_per_hr_TW\").sort_index()\n",
    "ax = hod_p.plot(marker=\"o\")\n",
    "ax.set_title(\"Time-weighted Net $/hr by Hour of Day\")\n",
    "ax.set_xlabel(\"Pickup Hour\")\n",
    "ax.set_ylabel(\"Net $/hr\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/hour_of_day_net_per_hr.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 4) Weekday vs Weekend (bar) ----------\n",
    "wkend = load_csv(\"plots/weekday_weekend.csv\")\n",
    "wkend_p = wkend.pivot(index=\"service_type\", columns=\"week_type\", values=\"net_per_hr_TW\")\n",
    "ax = wkend_p.plot(kind=\"bar\")\n",
    "ax.set_title(\"Time-weighted Net $/hr: Weekday vs Weekend\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Net $/hr\")\n",
    "ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/weekday_weekend_net_per_hr.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 5) Earnings composition (stacked per-trip) ----------\n",
    "comp = load_csv(\"plots/composition.csv\")\n",
    "comp = comp.set_index(\"service_type\")[[\"rev_per_trip\",\"nonfuel_per_trip\",\"fuel_per_trip\"]]\n",
    "ax = comp.plot(kind=\"bar\", stacked=True)\n",
    "ax.set_title(\"Per-trip Averages: Revenue vs Costs\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"USD per trip\")\n",
    "ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/composition_per_trip.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 6) Efficiency quantiles (bars with error bars) ----------\n",
    "effq = load_csv(\"plots/efficiency_quantiles.csv\")\n",
    "# Build error bars for net_per_mi using q25/median/q75\n",
    "effq = effq.set_index(\"service_type\")\n",
    "median = effq[\"net_per_mi_q50\"]\n",
    "err_low = median - effq[\"net_per_mi_q25\"]\n",
    "err_hi  = effq[\"net_per_mi_q75\"] - median\n",
    "ax = median.plot(kind=\"bar\", yerr=[err_low, err_hi], capsize=3)\n",
    "ax.set_title(\"Net per Mile (q25–median–q75)\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"USD per mile\")\n",
    "ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/net_per_mile_quantiles.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ======================================================\n",
    "# GEOSPATIAL (requires GeoPandas + shapely + fiona)\n",
    "#   pip install geopandas shapely fiona pyproj\n",
    "# ======================================================\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import LineString\n",
    "\n",
    "    TAXI_ZONES_PATH = \"taxi_zones/taxi_zones.geojson\"  # <- change if needed\n",
    "    zones = gpd.read_file(TAXI_ZONES_PATH)[[\"LocationID\",\"geometry\",\"zone\",\"borough\"]]\n",
    "\n",
    "    # ---------- 7) Zone choropleths: Yellow + HVFHV net/hr ----------\n",
    "    zone_y  = load_csv(\"plots/zone_yellow.csv\").rename(columns={\"PULocationID\":\"LocationID\"})\n",
    "    zone_hv = load_csv(\"plots/zone_hvfhv.csv\").rename(columns={\"PULocationID\":\"LocationID\"})\n",
    "\n",
    "    z_y = zones.merge(zone_y[[\"LocationID\",\"net_per_hr_TW\",\"trips\"]], on=\"LocationID\", how=\"left\")\n",
    "    z_h = zones.merge(zone_hv[[\"LocationID\",\"net_per_hr_TW\",\"trips\"]], on=\"LocationID\", how=\"left\")\n",
    "\n",
    "    ax = z_y.plot(column=\"net_per_hr_TW\", legend=True)\n",
    "    ax.set_title(\"Yellow: Net $/hr by Pickup Zone\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/img/map_zone_yellow_net_per_hr.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    ax = z_h.plot(column=\"net_per_hr_TW\", legend=True)\n",
    "    ax.set_title(\"HVFHV: Net $/hr by Pickup Zone\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/img/map_zone_hvfhv_net_per_hr.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # ---------- 8) Difference map: Yellow − HVFHV ----------\n",
    "    zone_diff = load_csv(\"plots/zone_diff_y_minus_hv.csv\")\n",
    "    zone_diff = zone_diff.rename(columns={\"PULocationID\":\"LocationID\", \"diff_yellow_minus_hv\":\"diff_net_per_hr\"})\n",
    "    z_diff = zones.merge(zone_diff[[\"LocationID\",\"diff_net_per_hr\"]], on=\"LocationID\", how=\"left\")\n",
    "\n",
    "    ax = z_diff.plot(column=\"diff_net_per_hr\", legend=True)\n",
    "    ax.set_title(\"Net $/hr Difference (Yellow − HVFHV) by Zone\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/img/map_zone_diff.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # ---------- 9) Pickup density (trips) ----------\n",
    "    zone_trips = load_csv(\"plots/zone_trips.csv\").rename(columns={\"PULocationID\":\"LocationID\"})\n",
    "    # choose a service to map (Yellow as default)\n",
    "    zt_y = zones.merge(zone_trips[zone_trips.service_type==\"yellow\"][[\"LocationID\",\"trips\"]],\n",
    "                       on=\"LocationID\", how=\"left\")\n",
    "    ax = zt_y.plot(column=\"trips\", legend=True)\n",
    "    ax.set_title(\"Pickup Density (Trips) — Yellow\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/img/map_zone_trips_yellow.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # ---------- 10) Desire lines (top flows) ----------\n",
    "    flows_y  = load_csv(\"plots/flows_top200_yellow.csv\")\n",
    "    flows_hv = load_csv(\"plots/flows_top200_hvfhv.csv\")\n",
    "\n",
    "    # centroids lookup\n",
    "    zones_cent = zones.copy()\n",
    "    zones_cent[\"centroid\"] = zones_cent.geometry.centroid\n",
    "    cent = zones_cent.set_index(\"LocationID\")[\"centroid\"].to_dict()\n",
    "\n",
    "    def flows_to_gdf(df_flows):\n",
    "        rows = []\n",
    "        for _, r in df_flows.iterrows():\n",
    "            pu, do, trips = int(r[\"PULocationID\"]), int(r[\"DOLocationID\"]), int(r[\"trips\"])\n",
    "            if pu in cent and do in cent:\n",
    "                line = LineString([cent[pu], cent[do]])\n",
    "                rows.append({\"PULocationID\": pu, \"DOLocationID\": do, \"trips\": trips, \"geometry\": line})\n",
    "        return gpd.GeoDataFrame(rows, geometry=\"geometry\", crs=zones.crs)\n",
    "\n",
    "    gdf_y  = flows_to_gdf(flows_y)\n",
    "    gdf_hv = flows_to_gdf(flows_hv)\n",
    "\n",
    "    # scale linewidths by trips (simple linear scaling)\n",
    "    lw_y  = (gdf_y[\"trips\"] / gdf_y[\"trips\"].max()) * 4 + 0.5\n",
    "    lw_hv = (gdf_hv[\"trips\"] / gdf_hv[\"trips\"].max()) * 4 + 0.5\n",
    "\n",
    "    base = zones.boundary.plot(linewidth=0.5)\n",
    "    gdf_y.plot(ax=base, linewidth=lw_y)\n",
    "    plt.title(\"Top 200 OD Flows — Yellow\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/img/flows_yellow.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    base = zones.boundary.plot(linewidth=0.5)\n",
    "    gdf_hv.plot(ax=base, linewidth=lw_hv)\n",
    "    plt.title(\"Top 200 OD Flows — HVFHV\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/img/flows_hvfhv.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # ---------- 11) Airport pickups ----------\n",
    "    ap = load_csv(\"plots/airport_pickups.csv\")\n",
    "    ap_p = ap.pivot(index=\"service_type\", columns=\"airport\", values=\"net_per_hr_TW\")\n",
    "    ax = ap_p.plot(kind=\"bar\")\n",
    "    ax.set_title(\"Airport Pickup Net $/hr (Time-weighted)\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Net $/hr\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/img/airport_pickups.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\nSaved maps to plots/img/*.png\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"\\n[Note] GeoPandas not installed — skipping map outputs.\")\n",
    "    print(\"Install with: pip install geopandas shapely fiona pyproj\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast30034-venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
