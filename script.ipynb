{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02a0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/30 19:57:21 WARN Utils: Your hostname, LAPTOP-E04ANIN1, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/08/30 19:57:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/30 19:57:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Starts a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Taxi vs Rideshare Profitability\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", False)   \n",
    "        .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "        .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"320\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")       \n",
    "        .config(\"spark.driver.memory\", \"6g\")                \n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Define months\n",
    "months = [\"2024-01\",\"2024-02\",\"2024-03\",\"2024-04\",\"2024-05\",\"2024-06\"]\n",
    "\n",
    "# Load in data files \n",
    "yellow_files = [f\"data/yellow/yellow_tripdata_{m}.parquet\" for m in months]\n",
    "fhvhv_files  = [f\"data/fhvhv/fhvhv_tripdata_{m}.parquet\"   for m in months]\n",
    "\n",
    "df_yellow = (\n",
    "    spark.read.parquet(*yellow_files)\n",
    "         .withColumn(\"service_type\", lit(\"yellow\"))\n",
    ")\n",
    "df_fhvhv = (\n",
    "    spark.read.parquet(*fhvhv_files)\n",
    "         .withColumn(\"service_type\", lit(\"hv_fhv\"))\n",
    ")\n",
    "\n",
    "# Merge\n",
    "df = df_yellow.unionByName(df_fhvhv, allowMissingColumns=True)\n",
    "\n",
    "# External tables \n",
    "electricity = spark.read.csv(\"data/external/electricity.csv\", header=True, inferSchema=True)\n",
    "fuel = spark.read.csv(\"data/external/fuel.csv\", header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c04cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, coalesce, unix_timestamp, when, lit, date_format, hour, dayofweek, broadcast\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Window\n",
    "import geopandas as gpd\n",
    "\n",
    "# Print size of raw data\n",
    "#print(f\"Raw data count: {df.count():,}\")\n",
    "#_prev = df.count()  # baseline for drop tracking\n",
    "\n",
    "# Reduce memory usage\n",
    "_needed = [\n",
    "    \"service_type\",\n",
    "    \"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\n",
    "    \"pickup_datetime\",\"dropoff_datetime\",\n",
    "    \"trip_distance\",\"trip_miles\",\"trip_time\",\n",
    "    \"PULocationID\",\"DOLocationID\",\n",
    "    \"passenger_count\",\"payment_type\",\n",
    "    \"fare_amount\",\"extra\",\"tip_amount\",\n",
    "    \"driver_pay\",\"tips\"\n",
    "]\n",
    "df = df.select([c for c in _needed if c in df.columns])\n",
    "\n",
    "# Standardise timestamps\n",
    "df = (\n",
    "    df.withColumn(\"pickup_ts\",  to_timestamp(coalesce(col(\"tpep_pickup_datetime\"),  col(\"pickup_datetime\"))))\n",
    "      .withColumn(\"dropoff_ts\", to_timestamp(coalesce(col(\"tpep_dropoff_datetime\"), col(\"dropoff_datetime\"))))\n",
    ")\n",
    "\n",
    "# Remove rows with null pickup or dropoff timestamps\n",
    "df = df.filter(col(\"pickup_ts\").isNotNull() & col(\"dropoff_ts\").isNotNull())\n",
    "#print(f\"[Null pickup/dropoff removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Standardise location IDs\n",
    "df = df.withColumn(\"month\", date_format(col(\"pickup_ts\"), \"yyyy-MM\"))\n",
    "\n",
    "# Standardise distance and keep positive distances only and not null\n",
    "df = (\n",
    "    df.withColumn(\"distance_mi\", coalesce(col(\"trip_distance\"), col(\"trip_miles\")).cast(DoubleType()))\n",
    "      .filter(col(\"distance_mi\").isNotNull() & (col(\"distance_mi\") > 0))\n",
    ")\n",
    "#print(f\"[Invalid distance (<=0 or null)] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Standardise trip time and not null\n",
    "df = df.withColumn(\n",
    "    \"trip_time_s\",\n",
    "    when(col(\"trip_time\").isNotNull(), col(\"trip_time\").cast(\"double\"))\n",
    "    .otherwise((unix_timestamp(col(\"dropoff_ts\")) - unix_timestamp(col(\"pickup_ts\"))).cast(\"double\"))\n",
    ")\n",
    "df = df.filter(col(\"trip_time_s\").isNotNull() & (col(\"trip_time_s\") > 0))\n",
    "#print(f\"[Invalid trip_time_s (<=0 or null)] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Deduplicate rows \n",
    "dedupe_key = [c for c in [\"service_type\",\"pickup_ts\",\"dropoff_ts\",\"PULocationID\",\"DOLocationID\",\"distance_mi\",\"trip_time_s\"] if c in df.columns]\n",
    "\n",
    "# If dedupe_key is empty, we won't deduplicate\n",
    "if dedupe_key:\n",
    "    w = Window.partitionBy([\"month\"] + dedupe_key).orderBy(F.lit(1))\n",
    "    df = df.withColumn(\"__rn\", F.row_number().over(w)).filter(col(\"__rn\") == 1).drop(\"__rn\")\n",
    "    #print(f\"[Exact duplicates removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Fixed Parameters\n",
    "CREDIT_CARD_FEE = 0.025 \n",
    "MAINTENANCE_COST_PER_MILE = 0.15\n",
    "MAINTENANCE_COST_PER_MILE_HV = 0.15\n",
    "\n",
    "# Payment type exists\n",
    "if \"payment_type\" not in df.columns:\n",
    "    df = df.withColumn(\"payment_type\", lit(None).cast(IntegerType()))\n",
    "\n",
    "# Time features \n",
    "df = (df\n",
    "    .withColumn(\"pickup_hour\", hour(col(\"pickup_ts\")))\n",
    "    .withColumn(\"pickup_dow\", dayofweek(col(\"pickup_ts\")))  \n",
    "    .withColumn(\"is_weekend\", (col(\"pickup_dow\").isin([1,7])).cast(\"boolean\"))\n",
    ")\n",
    "\n",
    "# Add revenue \n",
    "rev_yellow = coalesce(col(\"fare_amount\"), lit(0.0)) + coalesce(col(\"extra\"), lit(0.0)) + coalesce(col(\"tip_amount\"), lit(0.0))\n",
    "rev_hv     = coalesce(col(\"driver_pay\"), lit(0.0)) + coalesce(col(\"tips\"), lit(0.0))\n",
    "df = df.withColumn(\n",
    "    \"revenue\",\n",
    "    when(col(\"service_type\") == \"yellow\", rev_yellow).otherwise(rev_hv).cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Add costs\n",
    "maint_rate = when(col(\"service_type\") == \"yellow\",\n",
    "                  lit(MAINTENANCE_COST_PER_MILE)\n",
    "              ).otherwise(\n",
    "                  lit(MAINTENANCE_COST_PER_MILE_HV)\n",
    "              )\n",
    "df = df.withColumn(\"expense_maintenance\", (col(\"distance_mi\") * maint_rate).cast(DoubleType()))\n",
    "\n",
    "# Credit card fee \n",
    "df = df.withColumn(\n",
    "     \"expense_cc_processing\",\n",
    "    when((col(\"service_type\") == \"yellow\") & (col(\"payment_type\") == 1),\n",
    "         (lit(CREDIT_CARD_FEE) * col(\"revenue\")).cast(DoubleType()))\n",
    "    .otherwise(lit(0.0))\n",
    ")\n",
    "\n",
    "# Expenses pre-fuel \n",
    "df = df.withColumn(\n",
    "    \"expenses_nonfuel\",\n",
    "    (col(\"expense_maintenance\") + col(\"expense_cc_processing\")).cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Keep distance\n",
    "df = df.filter(col(\"distance_mi\") >= 0.1)\n",
    "#print(f\"[Distance < 0.1 mi removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Make sure within month range\n",
    "df = df.filter( (col(\"month\") >= \"2024-01\") & (col(\"month\") <= \"2024-06\") )\n",
    "#print(f\"[Outside Jan–Jun 2024 removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Keep duration >= 60 seconds\n",
    "df = df.filter(col(\"trip_time_s\") >= 60)\n",
    "#print(f\"[Trip time < 60 s removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Keep positive passenger count \n",
    "if \"passenger_count\" in df.columns:\n",
    "    df = df.filter(\n",
    "        when(col(\"service_type\") == \"yellow\", col(\"passenger_count\") > 0)\n",
    "        .otherwise(True)\n",
    "    )\n",
    "    #print(f\"[Non-positive passenger_count (Yellow) removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "TAXI_ZONES_PATH = \"data/taxi_zones/taxi_zones.shp\"\n",
    "zones_gdf = gpd.read_file(TAXI_ZONES_PATH)[[\"LocationID\"]]\n",
    "\n",
    "valid_ids = (\n",
    "    zones_gdf[\"LocationID\"]\n",
    "      .dropna()\n",
    "      .astype(\"int64\")\n",
    "      .unique()\n",
    "      .tolist()\n",
    ")\n",
    "# Valid TLC zone IDs \n",
    "for c in [\"PULocationID\", \"DOLocationID\"]:\n",
    "    if c in df.columns:\n",
    "        df = df.filter(col(c).isin(valid_ids))\n",
    "        #print(f\"[Invalid {c} removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Non-negative money fields \n",
    "money_ok = (\n",
    "    (coalesce(col(\"fare_amount\"), lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"extra\"),       lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"tip_amount\"),  lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"driver_pay\"),  lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"tips\"),        lit(0.0))  >= 0)\n",
    ")\n",
    "df = df.filter(money_ok)\n",
    "#print(f\"[Negative money fields removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Minimum initial fare for Yellow \n",
    "df = df.filter(\n",
    "    when(col(\"service_type\") == \"yellow\", coalesce(col(\"fare_amount\"), lit(0.0)) >= 2.50)\n",
    "    .otherwise(True)\n",
    ")\n",
    "#print(f\"[Yellow fare < $2.50 removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Pre-fuel profitability\n",
    "df = (df\n",
    "    .withColumn(\"active_hours\", (col(\"trip_time_s\") / 3600.0).cast(DoubleType()))\n",
    "    .withColumn(\"net_before_fuel\", (col(\"revenue\") - col(\"expenses_nonfuel\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_per_hr_before_fuel\", (col(\"net_before_fuel\") / col(\"active_hours\")).cast(DoubleType()))\n",
    "    .withColumn(\"mph\", (col(\"distance_mi\") / (col(\"trip_time_s\")/3600.0)).cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Single-pass outlier trim using 99.9% quantile\n",
    "stacked = (\n",
    "    df.select(\"service_type\", F.lit(\"distance_mi\").alias(\"metric\"), col(\"distance_mi\").alias(\"value\"))\n",
    "      .unionByName(df.select(\"service_type\", F.lit(\"trip_time_s\").alias(\"metric\"), col(\"trip_time_s\").alias(\"value\")))\n",
    "      .unionByName(df.select(\"service_type\", F.lit(\"revenue\").alias(\"metric\"),     col(\"revenue\").alias(\"value\")))\n",
    ")\n",
    "bounds = (\n",
    "    stacked.groupBy(\"service_type\", \"metric\")\n",
    "           .agg(F.expr(\"percentile_approx(value, 0.999, 10000)\").alias(\"p999\"))\n",
    ")\n",
    "df = (\n",
    "    df.alias(\"t\")\n",
    "      .join(bounds.alias(\"b1\").filter(col(\"b1.metric\") == \"distance_mi\")\n",
    "                 .select(col(\"service_type\").alias(\"s1\"), col(\"p999\").alias(\"p_d\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s1\")], how=\"left\")\n",
    "      .join(bounds.alias(\"b2\").filter(col(\"b2.metric\") == \"trip_time_s\")\n",
    "                 .select(col(\"service_type\").alias(\"s2\"), col(\"p999\").alias(\"p_t\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s2\")], how=\"left\")\n",
    "      .join(bounds.alias(\"b3\").filter(col(\"b3.metric\") == \"revenue\")\n",
    "                 .select(col(\"service_type\").alias(\"s3\"), col(\"p999\").alias(\"p_r\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s3\")], how=\"left\")\n",
    "      .filter( (col(\"distance_mi\") <= F.coalesce(col(\"p_d\"), lit(float(\"inf\")))) &\n",
    "               (col(\"trip_time_s\") <= F.coalesce(col(\"p_t\"), lit(float(\"inf\")))) &\n",
    "               (col(\"revenue\")     <= F.coalesce(col(\"p_r\"), lit(float(\"inf\")))) )\n",
    "      .drop(\"s1\",\"s2\",\"s3\",\"p_d\",\"p_t\",\"p_r\")\n",
    ")\n",
    "#print(f\"[Outliers (p99.9) removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "df = df.repartition(64, \"service_type\", \"month\")\n",
    "\n",
    "# Cap impossible speeds\n",
    "df = df.filter((col(\"mph\") >= 0) & (col(\"mph\") <= 120.0))\n",
    "#print(f\"[Impossible speeds (mph < 0 or > 120) removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Fuel and energy costs\n",
    "fuel = fuel.select(\"month\", \"price_per_gallon\").dropna()\n",
    "electricity = electricity.select(\"month\", \"price_usd_per_kwh\").dropna()\n",
    "\n",
    "# Join fuel and electricity prices\n",
    "df = (df.join(broadcast(fuel), on=\"month\", how=\"left\")\n",
    "        .join(broadcast(electricity), on=\"month\", how=\"left\")\n",
    ")\n",
    "# Remove if fuel or electricity price is missing\n",
    "df = df.filter(col(\"price_per_gallon\").isNotNull() & col(\"price_usd_per_kwh\").isNotNull())\n",
    "#print(f\"[Missing fuel/electricity price removed] dropped {_prev - (_after := df.count()):,} rows (from {_prev:,} to {_after:,})\"); _prev = _after\n",
    "\n",
    "# Energy assumptions according to EPA and AFDC \n",
    "MPG_FHV  = 27.0  \n",
    "MPG_TAXI = 16.0  \n",
    "\n",
    "KWH_YELLOW = 0.30\n",
    "KWH_FHV    = 0.30\n",
    "\n",
    "YELLOW_EV_PERCENT = 0.00  # Assuming no EVs in Yellow Taxi fleet \n",
    "FHV_EV_PERCENT    = 0.10  # Example share for HVFHV\n",
    "\n",
    "# Per-service parameters as columns \n",
    "df = (df\n",
    "    .withColumn(\n",
    "        \"mpg\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(MPG_TAXI))\n",
    "        .otherwise(lit(MPG_FHV)).cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"kwh_per_mile\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(KWH_YELLOW))\n",
    "        .otherwise(lit(KWH_FHV)).cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ev_share\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(YELLOW_EV_PERCENT))\n",
    "        .otherwise(lit(FHV_EV_PERCENT)).cast(DoubleType())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cost per mile (blend gas vs EV by ev_share)\n",
    "gas_cpm = (col(\"price_per_gallon\") / col(\"mpg\")).cast(DoubleType())\n",
    "ev_cpm  = (col(\"price_usd_per_kwh\") * col(\"kwh_per_mile\")).cast(DoubleType())\n",
    "\n",
    "df = (df\n",
    "    .withColumn(\n",
    "        \"energy_cost_per_mile\",\n",
    "        ((lit(1.0) - coalesce(col(\"ev_share\"), lit(0.0))) * coalesce(gas_cpm, lit(0.0))) +\n",
    "        (coalesce(col(\"ev_share\"), lit(0.0)) * coalesce(ev_cpm, lit(0.0)))\n",
    "    )\n",
    "    .withColumn(\"expense_fuel\", (col(\"distance_mi\") * col(\"energy_cost_per_mile\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_after_fuel\", (col(\"revenue\") - col(\"expenses_nonfuel\") - col(\"expense_fuel\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_per_hr_after_fuel\", (col(\"net_after_fuel\") / col(\"active_hours\")).cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Keep only relevant columns\n",
    "cols_keep = [\n",
    "    # Metadata\n",
    "    \"service_type\", \"month\", \"pickup_ts\", \"dropoff_ts\",\n",
    "    \"pickup_hour\", \"pickup_dow\", \"is_weekend\",\n",
    "    # Location IDs\n",
    "    \"PULocationID\", \"DOLocationID\",\n",
    "    # engineered trip metrics\n",
    "    \"distance_mi\", \"trip_time_s\", \"mph\", \"active_hours\",\n",
    "    # Feature engineering\n",
    "    \"revenue\", \"expenses_nonfuel\", \"expense_fuel\",\n",
    "    \"net_before_fuel\", \"net_after_fuel\",\n",
    "    \"net_per_hr_before_fuel\", \"net_per_hr_after_fuel\",\n",
    "    # Parameters\n",
    "    \"price_per_gallon\", \"price_usd_per_kwh\",\n",
    "    \"energy_cost_per_mile\", \"ev_share\", \"mpg\", \"kwh_per_mile\",\n",
    "]\n",
    "\n",
    "# Filter columns to keep only those that exist in the DataFrame\n",
    "cols_keep = [c for c in cols_keep if c in df.columns]\n",
    "\n",
    "# Print size after cleaning\n",
    "df = df.select(cols_keep)\n",
    "#print(f\"Cleaned data count: {df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17467e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "from pyspark.sql.functions import col, sum as ssum, count, round as sround, when, lit, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Testing\n",
    "FAST_MODE = False\n",
    "df_base = df.stat.sampleBy(\"service_type\", {\"yellow\":0.02,\"hv_fhv\":0.02}, seed=7) if FAST_MODE else df\n",
    "df_base = df_base.repartition(320, \"service_type\", \"month\")\n",
    "\n",
    "# Aggregation helpers\n",
    "sum_net   = ssum(\"net_after_fuel\").alias(\"sum_net\")\n",
    "sum_hours = ssum(\"active_hours\").alias(\"sum_hours\")\n",
    "\n",
    "def safe_net_per_hr(df_agg):\n",
    "    return df_agg.withColumn(\"net_per_hr_TW\", sround(F.when(col(\"sum_hours\") > 0, col(\"sum_net\")/col(\"sum_hours\")), 2))\n",
    "\n",
    "# Overall summary\n",
    "overall = safe_net_per_hr(\n",
    "    df_base.groupBy(\"service_type\").agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    ")\n",
    "\n",
    "# Figure 1 Composition \n",
    "\n",
    "# Earnings per trip\n",
    "comp = (\n",
    "    df_base.groupBy(\"service_type\")\n",
    "           .agg(\n",
    "               ssum(\"revenue\").alias(\"sum_rev\"),\n",
    "               ssum(\"expenses_nonfuel\").alias(\"sum_nonfuel\"),\n",
    "               ssum(\"expense_fuel\").alias(\"sum_fuel\"),\n",
    "               count(\"*\").alias(\"n_trips\")\n",
    "           )\n",
    "           .withColumn(\"rev_per_trip\",     sround(col(\"sum_rev\")/col(\"n_trips\"), 2))\n",
    "           .withColumn(\"nonfuel_per_trip\", sround(col(\"sum_nonfuel\")/col(\"n_trips\"), 2))\n",
    "           .withColumn(\"fuel_per_trip\",    sround(col(\"sum_fuel\")/col(\"n_trips\"), 2))\n",
    "           .select(\"service_type\",\"n_trips\",\"rev_per_trip\",\"nonfuel_per_trip\",\"fuel_per_trip\")\n",
    ")\n",
    "\n",
    "# Figure 2\n",
    "# Zone level relative advanatge \n",
    "zone_stats = (\n",
    "    df_base.groupBy(\"service_type\",\"PULocationID\")\n",
    "           .agg(\n",
    "               ssum(\"net_after_fuel\").alias(\"sum_net\"),\n",
    "               ssum(\"active_hours\").alias(\"sum_hours\"),\n",
    "               count(\"*\").alias(\"trips\")\n",
    "           )\n",
    "           .withColumn(\"net_per_hr_TW\", col(\"sum_net\")/col(\"sum_hours\"))\n",
    ")\n",
    "zone_y = zone_stats.filter(col(\"service_type\")==\"yellow\") \\\n",
    "                   .select(\"PULocationID\",\n",
    "                           col(\"net_per_hr_TW\").alias(\"y_hr\"),\n",
    "                           col(\"trips\").alias(\"y_trips\"))\n",
    "zone_h = zone_stats.filter(col(\"service_type\")==\"hv_fhv\") \\\n",
    "                   .select(\"PULocationID\",\n",
    "                           col(\"net_per_hr_TW\").alias(\"h_hr\"),\n",
    "                           col(\"trips\").alias(\"h_trips\"))\n",
    "zone_rel = (zone_y.join(zone_h, \"PULocationID\", \"outer\")\n",
    "                 .withColumn(\"total_trips\", F.coalesce(col(\"y_trips\"), F.lit(0)) + F.coalesce(col(\"h_trips\"), F.lit(0)))\n",
    "                 .withColumn(\"denom\", F.coalesce(col(\"y_hr\"), F.lit(0)) + F.coalesce(col(\"h_hr\"), F.lit(0)))\n",
    "                 .withColumn(\"share_diff\",\n",
    "                             F.when(col(\"denom\") > 0,\n",
    "                                    (F.coalesce(col(\"y_hr\"), F.lit(0)) - F.coalesce(col(\"h_hr\"), F.lit(0))) / col(\"denom\"))\n",
    "                              .otherwise(F.lit(None)))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07644c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, MultipleLocator\n",
    "from shapely.geometry import LineString\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import geopandas as gpd\n",
    "\n",
    "# Settings\n",
    "os.makedirs(\"plots/img\", exist_ok=True)\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (6.5, 4.2),\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"legend.fontsize\": 9,\n",
    "})\n",
    "# Color settings\n",
    "COLORS = {\"yellow\": \"#FFC000\", \"hv_fhv\": \"#2C7FB8\"}\n",
    "COST_NONFUEL = \"#8C8C8C\"\n",
    "COST_FUEL = \"#2CA25F\"\n",
    "NET_MARKER = \"#1a1a1a\"\n",
    "usd0 = FuncFormatter(lambda x, pos: f\"${x:,.0f}\")\n",
    "\n",
    "# Figure 1: Hour-of-day with 95% CI + overall means\n",
    "daily = (\n",
    "    df_base.withColumn(\"pickup_date\", to_date(col(\"pickup_ts\")))\n",
    "           .groupBy(\"service_type\", \"pickup_date\", \"pickup_hour\")\n",
    "           .agg(ssum(\"net_after_fuel\").alias(\"sum_net\"),\n",
    "                ssum(\"active_hours\").alias(\"sum_hours\"))\n",
    "           .withColumn(\"net_per_hr\", col(\"sum_net\")/col(\"sum_hours\"))\n",
    ")\n",
    "daily_pd = daily.select(\"service_type\",\"pickup_hour\",\"net_per_hr\").toPandas()\n",
    "g = daily_pd.groupby([\"service_type\",\"pickup_hour\"])[\"net_per_hr\"]\n",
    "hour_stats = (g.mean().rename(\"mean\").reset_index()\n",
    "              .merge(g.std(ddof=1).rename(\"std\").reset_index(), on=[\"service_type\",\"pickup_hour\"])\n",
    "              .merge(g.count().rename(\"n\").reset_index(), on=[\"service_type\",\"pickup_hour\"]))\n",
    "hour_stats[\"sem\"]  = hour_stats[\"std\"] / np.sqrt(hour_stats[\"n\"].clip(lower=1))\n",
    "hour_stats[\"ci95\"] = 1.96 * hour_stats[\"sem\"]\n",
    "\n",
    "overall_hr = (overall.select(\"service_type\",\"net_per_hr_TW\")\n",
    "                     .toPandas().set_index(\"service_type\")[\"net_per_hr_TW\"])\n",
    "\n",
    "# Plotting for Fig 1\n",
    "fig, ax = plt.subplots()\n",
    "for svc in [\"yellow\",\"hv_fhv\"]:\n",
    "    sub = hour_stats[hour_stats[\"service_type\"]==svc].sort_values(\"pickup_hour\")\n",
    "    x = sub[\"pickup_hour\"].values\n",
    "    m = sub[\"mean\"].values\n",
    "    c = COLORS[svc]\n",
    "    ax.plot(x, m, marker=\"o\", linewidth=2, markersize=3, color=c, label=(\"Yellow\" if svc==\"yellow\" else \"HVFHV\"))\n",
    "    ax.fill_between(x, m - sub[\"ci95\"].values, m + sub[\"ci95\"].values, color=c, alpha=0.18, linewidth=0)\n",
    "for svc in [\"yellow\",\"hv_fhv\"]:\n",
    "    if svc in overall_hr.index:\n",
    "        ax.axhline(overall_hr[svc], color=COLORS[svc], linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax.set_title(\"Time-weighted Net Profit by Hour of Day (Jan–Jun 2024)\")\n",
    "ax.set_xlabel(\"Pickup time\")\n",
    "ax.set_ylabel(\"Net profit per active hour\")\n",
    "ax.yaxis.set_major_formatter(usd0)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "ax.set_xticklabels([f\"{int(t):02d}:00\" for t in ax.get_xticks()])\n",
    "ax.grid(True, axis=\"y\", alpha=0.25)\n",
    "ax.legend(frameon=False, ncols=2, loc=\"upper center\", bbox_to_anchor=(0.5, -0.12))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/hour_of_day_TW_ci.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 2 Compisition Bar graph\n",
    "comp_pd = comp.toPandas().set_index(\"service_type\")[[\"rev_per_trip\",\"nonfuel_per_trip\",\"fuel_per_trip\"]]\n",
    "\n",
    "ax = comp_pd.plot(kind=\"bar\", stacked=True, title=\"Per-trip Revenue vs Costs\")\n",
    "ax.set_xlabel(\"Service type\")\n",
    "ax.set_ylabel(\"USD per trip\")\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda v, : f\"${v:,.0f}\")) \n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.axhline(0, linewidth=1) \n",
    "ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/composition.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Print overall summary\n",
    "print(\"\\nOVERALL (time-weighted net/hr):\")\n",
    "print(overall.select(\"service_type\",\"sum_net\",\"sum_hours\",\"trips\",\"net_per_hr_TW\").toPandas().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e12dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEOSPATIAL\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "# Load taxi zones shapefile\n",
    "TAXI_ZONES_PATH = \"data/taxi_zones/taxi_zones.shp\"\n",
    "zones = gpd.read_file(TAXI_ZONES_PATH)[[\"LocationID\",\"geometry\",\"zone\",\"borough\"]]\n",
    "zone_rel_pd = zone_rel.select(\"PULocationID\",\"share_diff\",\"total_trips\").toPandas() \\\n",
    "                      .rename(columns={\"PULocationID\":\"LocationID\"})\n",
    "gmap = zones.merge(zone_rel_pd, on=\"LocationID\", how=\"left\")\n",
    "\n",
    "# Mask zones with very few trips\n",
    "LOW_TRIPS = 100\n",
    "gmap[\"masked\"] = gmap[\"total_trips\"].fillna(0) < LOW_TRIPS\n",
    "plot_col = gmap[\"share_diff\"].where(~gmap[\"masked\"], np.nan)\n",
    "\n",
    "# Plotting for Fig 3\n",
    "fig, ax = plt.subplots(figsize=(6.5, 6.5))\n",
    "norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "gmap.assign(plot_val=plot_col).plot(\n",
    "    column=\"plot_val\", cmap=\"RdBu_r\", norm=norm,\n",
    "    linewidth=0.2, edgecolor=\"#aaaaaa\", ax=ax,\n",
    "    legend=True, legend_kwds={\"label\": \"Share difference (Y−H)/(Y+H)\", \"shrink\": 0.6}\n",
    ")\n",
    "gmap[gmap[\"masked\"]].plot(ax=ax, color=\"#DDDDDD\", linewidth=0.2, edgecolor=\"#aaaaaa\", label=f\"< {LOW_TRIPS} trips\")\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Relative Advantage by Pickup Zone (Jan–Jun 2024)\")\n",
    "from matplotlib.patches import Patch\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "if f\"< {LOW_TRIPS} trips\" not in labels:\n",
    "    handles.append(Patch(facecolor=\"#DDDDDD\", edgecolor=\"#aaaaaa\", label=f\"< {LOW_TRIPS} trips\"))\n",
    "ax.legend(handles=handles, frameon=False, loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/map_share_diff.png\", dpi=220, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Model 1\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum as ssum, count, to_date\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Aggregate to (service, zone, date, hour)\n",
    "_ts = next((c for c in [\"pickup_ts\",\"pickup_datetime\",\"tpep_pickup_datetime\",\"lpep_pickup_datetime\"]\n",
    "            if c in df.columns), None)\n",
    "assert _ts is not None, \"No pickup timestamp column found.\"\n",
    "\n",
    "by_shz = (\n",
    "    df.select(\"service_type\",\"PULocationID\",\"pickup_hour\",\n",
    "              \"net_after_fuel\",\"active_hours\",\"price_per_gallon\",\"price_usd_per_kwh\", _ts)\n",
    "      .withColumn(\"pickup_date\", to_date(col(_ts)))\n",
    "      .groupBy(\"service_type\",\"PULocationID\",\"pickup_date\",\"pickup_hour\")\n",
    "      .agg(\n",
    "          ssum(\"net_after_fuel\").alias(\"sum_net\"),\n",
    "          ssum(\"active_hours\").alias(\"sum_hours\"),\n",
    "          count(\"*\").alias(\"trips\"),\n",
    "          F.first(\"price_per_gallon\").alias(\"price_per_gallon\"),\n",
    "          F.first(\"price_usd_per_kwh\").alias(\"price_usd_per_kwh\")\n",
    "      )\n",
    "      .filter(col(\"sum_hours\") > 0)\n",
    "      .withColumn(\"net_hr\",  col(\"sum_net\")/col(\"sum_hours\"))\n",
    "      .withColumn(\"trips_hr\", col(\"trips\")/col(\"sum_hours\"))\n",
    "      .withColumn(\"dow\", F.dayofweek(\"pickup_date\"))\n",
    "      .withColumn(\"is_weekend\", F.col(\"dow\").isin(1,7).cast(\"int\"))\n",
    "      .withColumn(\"month_ix\", F.month(\"pickup_date\"))\n",
    "      .withColumn(\"ym\", F.date_format(\"pickup_date\",\"yyyy-MM\"))\n",
    ")\n",
    "\n",
    "# Pair services on the same zone–date–hour and build Δ & weights\n",
    "wide = (\n",
    "    by_shz.groupBy(\"PULocationID\",\"pickup_date\",\"pickup_hour\",\"dow\",\"is_weekend\",\"month_ix\",\"ym\",\n",
    "                   \"price_per_gallon\",\"price_usd_per_kwh\")\n",
    "          .pivot(\"service_type\", [\"yellow\",\"hv_fhv\"])\n",
    "          .agg(F.first(\"net_hr\").alias(\"net_hr\"),\n",
    "               F.first(\"trips_hr\").alias(\"trips_hr\"),\n",
    "               F.first(\"sum_hours\").alias(\"sum_hours\"))\n",
    "          .fillna(0.0)\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "for old, new in [(\"yellow_net_hr\",\"y_net_hr\"), (\"hv_fhv_net_hr\",\"h_net_hr\"),\n",
    "                 (\"yellow_trips_hr\",\"y_trips_hr\"), (\"hv_fhv_trips_hr\",\"h_trips_hr\"),\n",
    "                 (\"yellow_sum_hours\",\"y_hours\"), (\"hv_fhv_sum_hours\",\"h_hours\")]:\n",
    "    if old in wide.columns:\n",
    "        wide = wide.withColumnRenamed(old, new)\n",
    "\n",
    "feat = (wide\n",
    "        .withColumn(\"delta\", col(\"y_net_hr\") - col(\"h_net_hr\"))\n",
    "        .withColumn(\"w_hours\", col(\"y_hours\") + col(\"h_hours\"))  \n",
    ")\n",
    "\n",
    "# Time-aware split\n",
    "train = feat.filter(col(\"ym\") <= \"2024-05\").cache()\n",
    "test  = feat.filter(col(\"ym\") == \"2024-06\").cache()\n",
    "\n",
    "# Encode zone and assemble features\n",
    "idx  = StringIndexer(inputCol=\"PULocationID\", outputCol=\"zone_idx\", handleInvalid=\"keep\")\n",
    "ohe  = OneHotEncoder(inputCols=[\"zone_idx\"], outputCols=[\"zone_ohe\"], dropLast=False)\n",
    "\n",
    "# Features\n",
    "va = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"pickup_hour\",\"dow\",\"is_weekend\",\"month_ix\",\n",
    "        \"y_trips_hr\",\"h_trips_hr\",\n",
    "        \"price_per_gallon\",\"price_usd_per_kwh\",\n",
    "        \"zone_ohe\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Weighted ridge regression on Δ\n",
    "lin = LinearRegression(\n",
    "    labelCol=\"delta\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"w_hours\",  \n",
    "    elasticNetParam=0.0,   \n",
    "    regParam=0.2,\n",
    "    maxIter=200\n",
    ") \n",
    "\n",
    "# Pipeline\n",
    "pipe = Pipeline(stages=[idx, ohe, va, lin]).fit(train)\n",
    "pred = pipe.transform(test)\n",
    "\n",
    "# Evaluate\n",
    "pred_eval = pred.withColumn(\n",
    "    \"abs_err\", F.abs(col(\"delta\") - col(\"prediction\"))\n",
    ").withColumn(\n",
    "    \"sq_err\", (col(\"delta\") - col(\"prediction\"))**2\n",
    ")\n",
    "\n",
    "# Weighted MAE\n",
    "wmae = (\n",
    "    pred_eval.withColumn(\"w_abs_err\", col(\"w_hours\") * col(\"abs_err\"))\n",
    "             .agg((F.sum(\"w_abs_err\") / F.sum(\"w_hours\")).alias(\"wmae\"))\n",
    "             .collect()[0][\"wmae\"]\n",
    ")\n",
    "\n",
    "# Weighted RMSE \n",
    "wrmse = (\n",
    "    pred_eval.withColumn(\"w_sq_err\", col(\"w_hours\") * col(\"sq_err\"))\n",
    "             .agg((F.sqrt(F.sum(\"w_sq_err\") / F.sum(\"w_hours\"))).alias(\"wrmse\"))\n",
    "             .collect()[0][\"wrmse\"]\n",
    ")\n",
    "\n",
    "print(f\"Model 1 (Ridge Δ) — June: wMAE={wmae:.2f}, wRMSE={wrmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2080c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning plot 1 \n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, os\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "os.makedirs(\"plots/img\", exist_ok=True)\n",
    "\n",
    "m1 = pred.select(\"delta\",\"prediction\").toPandas().dropna()\n",
    "\n",
    "# Bin by predicted difference\n",
    "m1[\"bin\"] = pd.qcut(m1[\"prediction\"], 10, duplicates=\"drop\")\n",
    "g = m1.groupby(\"bin\", observed=True)\n",
    "cal = pd.DataFrame({\n",
    "    \"pred_mean\": g[\"prediction\"].mean().values,\n",
    "    \"act_mean\":  g[\"delta\"].mean().values,\n",
    "    \"n\":         g.size().values\n",
    "}).sort_values(\"pred_mean\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6.8,4.2))\n",
    "plt.plot(cal[\"pred_mean\"], cal[\"act_mean\"], marker=\"o\", linewidth=2, label=\"Model 1 — Ridge\")\n",
    "lim = float(np.ceil(max(np.abs(cal[[\"pred_mean\",\"act_mean\"]]).to_numpy().ravel())/5)*5) or 10.0\n",
    "plt.plot([-lim, lim], [-lim, lim], \"k--\", linewidth=1, label=\"Perfect calibration (y=x)\")\n",
    "plt.xlabel(\"Predicted Δ  (bin mean)  [USD/hr]\")\n",
    "plt.ylabel(\"Actual Δ     (bin mean)  [USD/hr]\")\n",
    "plt.title(\"Calibration by prediction decile — Model 1 (Ridge), June\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.25)\n",
    "plt.legend(frameon=False, loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/model1_calibration_deciles.png\", dpi=200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning model 2\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Using the same train/test split and features as Model 1, but with GBT\n",
    "idx = StringIndexer(inputCol=\"PULocationID\", outputCol=\"zone_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "va = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"pickup_hour\",\"dow\",\"is_weekend\",\"month_ix\",\n",
    "        \"price_per_gallon\",\"price_usd_per_kwh\",\n",
    "        \"zone_idx\"             \n",
    "    ],\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Index categorical features\n",
    "vx = VectorIndexer(inputCol=\"features_raw\", outputCol=\"features\", maxCategories=300, handleInvalid=\"keep\")\n",
    "\n",
    "# GBT regressor on Δ\n",
    "gbt = GBTRegressor(\n",
    "    labelCol=\"delta\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=6,          \n",
    "    maxBins=512,\n",
    "    maxIter=120,        \n",
    "    stepSize=0.1,       \n",
    "    subsamplingRate=0.8, \n",
    "    seed=7\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipe_gbt = Pipeline(stages=[idx, va, vx, gbt]).fit(train)\n",
    "pred_gbt = pipe_gbt.transform(test).cache()\n",
    "\n",
    "# Evaluate\n",
    "den = pred_gbt.agg(F.sum(\"w_hours\").alias(\"W\")).first()[\"W\"]\n",
    "\n",
    "w_mae = (pred_gbt\n",
    "         .select((F.col(\"w_hours\")*F.abs(F.col(\"delta\")-F.col(\"prediction\"))).alias(\"w_ae\"))\n",
    "         .agg(F.sum(\"w_ae\")).first()[0] / den)\n",
    "\n",
    "w_rmse = ((pred_gbt\n",
    "           .select((F.col(\"w_hours\")*F.pow(F.col(\"delta\")-F.col(\"prediction\"),2)).alias(\"w_se\"))\n",
    "           .agg(F.sum(\"w_se\")).first()[0] / den) ** 0.5)\n",
    "\n",
    "print(f\"Model 2 (GBT Δ) — June: wMAE={w_mae:.2f}, wRMSE={w_rmse:.2f}\")\n",
    "\n",
    "# Simple baseline: mean by hour of day\n",
    "base = (train.groupBy(\"pickup_hour\")\n",
    "              .agg(F.avg(\"delta\").alias(\"dbar\")))\n",
    "pred_base = (test.join(base, \"pickup_hour\", \"left\")\n",
    "                 .fillna({\"dbar\": float(train.agg(F.avg('delta')).first()[0])})\n",
    "                 .withColumn(\"pred_base\", F.col(\"dbar\")))\n",
    "b_mae = (pred_base\n",
    "         .select((F.col(\"w_hours\")*F.abs(F.col(\"delta\")-F.col(\"pred_base\"))).alias(\"w_ae\"))\n",
    "         .agg(F.sum(\"w_ae\")).first()[0] / den)\n",
    "b_rmse = ((pred_base\n",
    "           .select((F.col(\"w_hours\")*F.pow(F.col(\"delta\")-F.col(\"pred_base\"),2)).alias(\"w_se\"))\n",
    "           .agg(F.sum(\"w_se\")).first()[0] / den) ** 0.5)\n",
    "print(f\"Baseline (hour mean) — June: wMAE={b_mae:.2f}, wRMSE={b_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50419a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning plot 2\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "os.makedirs(\"plots/img\", exist_ok=True)\n",
    "usd0 = FuncFormatter(lambda x, pos: f\"${x:,.0f}\")\n",
    "\n",
    "# Calibration plot by decile\n",
    "w = Window.orderBy(F.col(\"prediction\"))\n",
    "pred_bins = pred_gbt.withColumn(\"bin\", F.ntile(10).over(w))\n",
    "\n",
    "# Making the bin stats table\n",
    "bin_stats = (pred_bins.groupBy(\"bin\")\n",
    "    .agg(\n",
    "        F.sum(\"w_hours\").alias(\"w\"),\n",
    "        (F.sum(F.col(\"w_hours\")*F.col(\"prediction\"))/F.sum(\"w_hours\")).alias(\"pred_mean\"),\n",
    "        (F.sum(F.col(\"w_hours\")*F.col(\"delta\"))/F.sum(\"w_hours\")).alias(\"actual_mean\"),\n",
    "        F.min(\"prediction\").alias(\"pred_min\"),\n",
    "        F.max(\"prediction\").alias(\"pred_max\")\n",
    "    )\n",
    "    .orderBy(\"bin\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.6, 4.6))\n",
    "ax.plot(bin_stats[\"pred_mean\"], bin_stats[\"actual_mean\"],\n",
    "        marker=\"o\", linewidth=2.2, label=\"Model 2 — GBT\")\n",
    "# y=x line\n",
    "lims = [min(bin_stats[\"pred_mean\"].min(), bin_stats[\"actual_mean\"].min()),\n",
    "        max(bin_stats[\"pred_mean\"].max(), bin_stats[\"actual_mean\"].max())]\n",
    "pad = (lims[1]-lims[0])*0.05\n",
    "ax.plot([lims[0]-pad, lims[1]+pad], [lims[0]-pad, lims[1]+pad],\n",
    "        linestyle=\"--\", color=\"k\", linewidth=1.2, label=\"Perfect calibration (y=x)\")\n",
    "\n",
    "ax.set_title(\"Calibration by prediction decile — Model 2 (GBT Δ), June\")\n",
    "ax.set_xlabel(\"Predicted Δ  (bin mean)  [USD/hr]\")\n",
    "ax.set_ylabel(\"Actual Δ  (bin mean)  [USD/hr]\")\n",
    "ax.yaxis.set_major_formatter(usd0)\n",
    "ax.xaxis.set_major_formatter(usd0)\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(frameon=False, loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/gbt_calibration.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Feature importance plot\n",
    "gbt_model = pipe_gbt.stages[-1]  \n",
    "feat_names = [\"pickup_hour\",\"dow\",\"is_weekend\",\"month_ix\",\n",
    "              \"price_per_gallon\",\"price_usd_per_kwh\",\"zone_idx\"]\n",
    "imp = np.array(gbt_model.featureImportances.toArray())\n",
    "order = np.argsort(imp)\n",
    "fig, ax = plt.subplots(figsize=(7.2, 4.6))\n",
    "ax.barh(np.array(feat_names)[order], imp[order])\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.set_xlabel(\"Importance (gain)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/gbt_feature_importance.png\", dpi=200)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast30034-venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
