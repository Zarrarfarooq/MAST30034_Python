{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02a0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/25 18:55:42 WARN Utils: Your hostname, LAPTOP-E04ANIN1, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/08/25 18:55:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/25 18:55:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Starts a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Taxi vs Rideshare Profitability\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", False)   \n",
    "        .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "        .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"320\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")       \n",
    "        .config(\"spark.driver.memory\", \"6g\")                \n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Define months\n",
    "months = [\"2024-01\",\"2024-02\",\"2024-03\",\"2024-04\",\"2024-05\",\"2024-06\"]\n",
    "\n",
    "# Load in data files \n",
    "yellow_files = [f\"data/yellow/yellow_tripdata_{m}.parquet\" for m in months]\n",
    "fhvhv_files  = [f\"data/fhvhv/fhvhv_tripdata_{m}.parquet\"   for m in months]\n",
    "\n",
    "df_yellow = (\n",
    "    spark.read.parquet(*yellow_files)\n",
    "         .withColumn(\"service_type\", lit(\"yellow\"))\n",
    ")\n",
    "df_fhvhv = (\n",
    "    spark.read.parquet(*fhvhv_files)\n",
    "         .withColumn(\"service_type\", lit(\"hv_fhv\"))\n",
    ")\n",
    "\n",
    "# Merge\n",
    "df = df_yellow.unionByName(df_fhvhv, allowMissingColumns=True)\n",
    "\n",
    "# External tables \n",
    "electricity = spark.read.csv(\"data/external/electricity.csv\", header=True, inferSchema=True)\n",
    "fuel = spark.read.csv(\"data/external/fuel.csv\", header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c04cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data count: 141,196,761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=======================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data count: 132,803,826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess data\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, coalesce, unix_timestamp, when, lit, date_format, hour, dayofweek, broadcast\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Window\n",
    "import geopandas as gpd\n",
    "\n",
    "# Print size of raw data\n",
    "print(f\"Raw data count: {df.count():,}\")\n",
    "\n",
    "# Reduce memory usage\n",
    "_needed = [\n",
    "    \"service_type\",\n",
    "    \"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\n",
    "    \"pickup_datetime\",\"dropoff_datetime\",\n",
    "    \"trip_distance\",\"trip_miles\",\"trip_time\",\n",
    "    \"PULocationID\",\"DOLocationID\",\n",
    "    \"passenger_count\",\"payment_type\",\n",
    "    \"fare_amount\",\"extra\",\"tip_amount\",\n",
    "    \"driver_pay\",\"tips\"\n",
    "]\n",
    "df = df.select([c for c in _needed if c in df.columns])\n",
    "\n",
    "# Standardise timestamps\n",
    "df = (\n",
    "    df.withColumn(\"pickup_ts\",  to_timestamp(coalesce(col(\"tpep_pickup_datetime\"),  col(\"pickup_datetime\"))))\n",
    "      .withColumn(\"dropoff_ts\", to_timestamp(coalesce(col(\"tpep_dropoff_datetime\"), col(\"dropoff_datetime\"))))\n",
    ")\n",
    "\n",
    "# Remove rows with null pickup or dropoff timestamps\n",
    "df = df.filter(col(\"pickup_ts\").isNotNull() & col(\"dropoff_ts\").isNotNull())\n",
    "\n",
    "# Standardise location IDs\n",
    "df = df.withColumn(\"month\", date_format(col(\"pickup_ts\"), \"yyyy-MM\"))\n",
    "\n",
    "# Standardise distance and keep positive distances only and not null\n",
    "df = (\n",
    "    df.withColumn(\"distance_mi\", coalesce(col(\"trip_distance\"), col(\"trip_miles\")).cast(DoubleType()))\n",
    "      .filter(col(\"distance_mi\").isNotNull() & (col(\"distance_mi\") > 0))\n",
    ")\n",
    "\n",
    "# Standardise trip time and not null\n",
    "df = df.withColumn(\n",
    "    \"trip_time_s\",\n",
    "    when(col(\"trip_time\").isNotNull(), col(\"trip_time\").cast(\"double\"))\n",
    "    .otherwise((unix_timestamp(col(\"dropoff_ts\")) - unix_timestamp(col(\"pickup_ts\"))).cast(\"double\"))\n",
    ")\n",
    "df = df.filter(col(\"trip_time_s\").isNotNull() & (col(\"trip_time_s\") > 0))\n",
    "\n",
    "# Deduplicate rows \n",
    "dedupe_key = [c for c in [\"service_type\",\"pickup_ts\",\"dropoff_ts\",\"PULocationID\",\"DOLocationID\",\"distance_mi\",\"trip_time_s\"] if c in df.columns]\n",
    "\n",
    "# If dedupe_key is empty, we won't deduplicate\n",
    "if dedupe_key:\n",
    "    w = Window.partitionBy([\"month\"] + dedupe_key).orderBy(F.lit(1))\n",
    "    df = df.withColumn(\"__rn\", F.row_number().over(w)).filter(col(\"__rn\") == 1).drop(\"__rn\")\n",
    "\n",
    "# Fixed Parameters\n",
    "CREDIT_CARD_FEE = 0.025 \n",
    "MAINTENANCE_COST_PER_MILE = 0.15\n",
    "MAINTENANCE_COST_PER_MILE_HV = 0.15\n",
    "\n",
    "# Payment type exists\n",
    "if \"payment_type\" not in df.columns:\n",
    "    df = df.withColumn(\"payment_type\", lit(None).cast(IntegerType()))\n",
    "\n",
    "# Time features \n",
    "df = (df\n",
    "    .withColumn(\"pickup_hour\", hour(col(\"pickup_ts\")))\n",
    "    .withColumn(\"pickup_dow\", dayofweek(col(\"pickup_ts\")))  \n",
    "    .withColumn(\"is_weekend\", (col(\"pickup_dow\").isin([1,7])).cast(\"boolean\"))\n",
    ")\n",
    "\n",
    "# Add revenue \n",
    "rev_yellow = coalesce(col(\"fare_amount\"), lit(0.0)) + coalesce(col(\"extra\"), lit(0.0)) + coalesce(col(\"tip_amount\"), lit(0.0))\n",
    "rev_hv     = coalesce(col(\"driver_pay\"), lit(0.0)) + coalesce(col(\"tips\"), lit(0.0))\n",
    "df = df.withColumn(\n",
    "    \"revenue\",\n",
    "    when(col(\"service_type\") == \"yellow\", rev_yellow).otherwise(rev_hv).cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Add costs\n",
    "maint_rate = when(col(\"service_type\") == \"yellow\",\n",
    "                  lit(MAINTENANCE_COST_PER_MILE)\n",
    "              ).otherwise(\n",
    "                  lit(MAINTENANCE_COST_PER_MILE_HV)\n",
    "              )\n",
    "df = df.withColumn(\"expense_maintenance\", (col(\"distance_mi\") * maint_rate).cast(DoubleType()))\n",
    "\n",
    "# Credit card fee \n",
    "df = df.withColumn(\n",
    "     \"expense_cc_processing\",\n",
    "    when((col(\"service_type\") == \"yellow\") & (col(\"payment_type\") == 1),\n",
    "         (lit(CREDIT_CARD_FEE) * col(\"revenue\")).cast(DoubleType()))\n",
    "    .otherwise(lit(0.0))\n",
    ")\n",
    "\n",
    "# Expenses pre-fuel \n",
    "df = df.withColumn(\n",
    "    \"expenses_nonfuel\",\n",
    "    (col(\"expense_maintenance\") + col(\"expense_cc_processing\")).cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Keep distance\n",
    "df = df.filter(col(\"distance_mi\") >= 0.1)\n",
    "\n",
    "# Make sure within month range\n",
    "df = df.filter( (col(\"month\") >= \"2024-01\") & (col(\"month\") <= \"2024-06\") )\n",
    "\n",
    "# Keep duration >= 60 seconds\n",
    "df = df.filter(col(\"trip_time_s\") >= 60)\n",
    "\n",
    "# Keep positive passenger count \n",
    "if \"passenger_count\" in df.columns:\n",
    "    df = df.filter(\n",
    "        when(col(\"service_type\") == \"yellow\", col(\"passenger_count\") > 0)\n",
    "        .otherwise(True)\n",
    "    )\n",
    "\n",
    "TAXI_ZONES_PATH = \"data/taxi_zones/taxi_zones.shp\"\n",
    "zones_gdf = gpd.read_file(TAXI_ZONES_PATH)[[\"LocationID\"]]\n",
    "\n",
    "valid_ids = (\n",
    "    zones_gdf[\"LocationID\"]\n",
    "      .dropna()\n",
    "      .astype(\"int64\")\n",
    "      .unique()\n",
    "      .tolist()\n",
    ")\n",
    "# Valid TLC zone IDs \n",
    "for c in [\"PULocationID\", \"DOLocationID\"]:\n",
    "    if c in df.columns:\n",
    "        df = df.filter(col(c).isin(valid_ids))\n",
    "\n",
    "# Non-negative money fields \n",
    "money_ok = (\n",
    "    (coalesce(col(\"fare_amount\"), lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"extra\"),       lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"tip_amount\"),  lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"driver_pay\"),  lit(0.0))  >= 0) &\n",
    "    (coalesce(col(\"tips\"),        lit(0.0))  >= 0)\n",
    ")\n",
    "df = df.filter(money_ok)\n",
    "\n",
    "# Minimum initial fare for Yellow \n",
    "df = df.filter(\n",
    "    when(col(\"service_type\") == \"yellow\", coalesce(col(\"fare_amount\"), lit(0.0)) >= 1.50)\n",
    "    .otherwise(True)\n",
    ")\n",
    "\n",
    "\n",
    "# Pre-fuel profitability\n",
    "df = (df\n",
    "    .withColumn(\"active_hours\", (col(\"trip_time_s\") / 3600.0).cast(DoubleType()))\n",
    "    .withColumn(\"net_before_fuel\", (col(\"revenue\") - col(\"expenses_nonfuel\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_per_hr_before_fuel\", (col(\"net_before_fuel\") / col(\"active_hours\")).cast(DoubleType()))\n",
    "    .withColumn(\"mph\", (col(\"distance_mi\") / (col(\"trip_time_s\")/3600.0)).cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Single-pass outlier trim using 99.9% quantile\n",
    "stacked = (\n",
    "    df.select(\"service_type\", F.lit(\"distance_mi\").alias(\"metric\"), col(\"distance_mi\").alias(\"value\"))\n",
    "      .unionByName(df.select(\"service_type\", F.lit(\"trip_time_s\").alias(\"metric\"), col(\"trip_time_s\").alias(\"value\")))\n",
    "      .unionByName(df.select(\"service_type\", F.lit(\"revenue\").alias(\"metric\"),     col(\"revenue\").alias(\"value\")))\n",
    ")\n",
    "bounds = (\n",
    "    stacked.groupBy(\"service_type\", \"metric\")\n",
    "           .agg(F.expr(\"percentile_approx(value, 0.999, 10000)\").alias(\"p999\"))\n",
    ")\n",
    "df = (\n",
    "    df.alias(\"t\")\n",
    "      .join(bounds.alias(\"b1\").filter(col(\"b1.metric\") == \"distance_mi\")\n",
    "                 .select(col(\"service_type\").alias(\"s1\"), col(\"p999\").alias(\"p_d\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s1\")], how=\"left\")\n",
    "      .join(bounds.alias(\"b2\").filter(col(\"b2.metric\") == \"trip_time_s\")\n",
    "                 .select(col(\"service_type\").alias(\"s2\"), col(\"p999\").alias(\"p_t\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s2\")], how=\"left\")\n",
    "      .join(bounds.alias(\"b3\").filter(col(\"b3.metric\") == \"revenue\")\n",
    "                 .select(col(\"service_type\").alias(\"s3\"), col(\"p999\").alias(\"p_r\")),\n",
    "            on=[col(\"t.service_type\") == col(\"s3\")], how=\"left\")\n",
    "      .filter( (col(\"distance_mi\") <= F.coalesce(col(\"p_d\"), lit(float(\"inf\")))) &\n",
    "               (col(\"trip_time_s\") <= F.coalesce(col(\"p_t\"), lit(float(\"inf\")))) &\n",
    "               (col(\"revenue\")     <= F.coalesce(col(\"p_r\"), lit(float(\"inf\")))) )\n",
    "      .drop(\"s1\",\"s2\",\"s3\",\"p_d\",\"p_t\",\"p_r\")\n",
    ")\n",
    "\n",
    "df = df.repartition(64, \"service_type\", \"month\")\n",
    "\n",
    "# Cap impossible speeds\n",
    "df = df.filter((col(\"mph\") >= 0) & (col(\"mph\") <= 120.0))\n",
    "\n",
    "\n",
    "# Fuel and energy costs\n",
    "fuel = fuel.select(\"month\", \"price_per_gallon\").dropna()\n",
    "electricity = electricity.select(\"month\", \"price_usd_per_kwh\").dropna()\n",
    "\n",
    "# Join fuel and electricity prices\n",
    "df = (df.join(broadcast(fuel), on=\"month\", how=\"left\")\n",
    "        .join(broadcast(electricity), on=\"month\", how=\"left\")\n",
    ")\n",
    "# Remove if fuel or electricity price is missing\n",
    "df = df.filter(col(\"price_per_gallon\").isNotNull() & col(\"price_usd_per_kwh\").isNotNull())\n",
    "\n",
    "# Energy assumptions according to EPA and AFDC \n",
    "MPG_FHV  = 27.0  \n",
    "MPG_TAXI = 16.0  \n",
    "\n",
    "KWH_YELLOW = 0.30\n",
    "KWH_FHV    = 0.30\n",
    "\n",
    "YELLOW_EV_PERCENT = 0.00  # Assuming no EVs in Yellow Taxi fleet \n",
    "FHV_EV_PERCENT    = 0.10  # Example share for HVFHV\n",
    "\n",
    "# Per-service parameters as columns \n",
    "df = (df\n",
    "    .withColumn(\n",
    "        \"mpg\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(MPG_TAXI))\n",
    "        .otherwise(lit(MPG_FHV)).cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"kwh_per_mile\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(KWH_YELLOW))\n",
    "        .otherwise(lit(KWH_FHV)).cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ev_share\",\n",
    "        when(col(\"service_type\") == \"yellow\", lit(YELLOW_EV_PERCENT))\n",
    "        .otherwise(lit(FHV_EV_PERCENT)).cast(DoubleType())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cost per mile (blend gas vs EV by ev_share)\n",
    "gas_cpm = (col(\"price_per_gallon\") / col(\"mpg\")).cast(DoubleType())\n",
    "ev_cpm  = (col(\"price_usd_per_kwh\") * col(\"kwh_per_mile\")).cast(DoubleType())\n",
    "\n",
    "df = (df\n",
    "    .withColumn(\n",
    "        \"energy_cost_per_mile\",\n",
    "        ((lit(1.0) - coalesce(col(\"ev_share\"), lit(0.0))) * coalesce(gas_cpm, lit(0.0))) +\n",
    "        (coalesce(col(\"ev_share\"), lit(0.0)) * coalesce(ev_cpm, lit(0.0)))\n",
    "    )\n",
    "    .withColumn(\"expense_fuel\", (col(\"distance_mi\") * col(\"energy_cost_per_mile\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_after_fuel\", (col(\"revenue\") - col(\"expenses_nonfuel\") - col(\"expense_fuel\")).cast(DoubleType()))\n",
    "    .withColumn(\"net_per_hr_after_fuel\", (col(\"net_after_fuel\") / col(\"active_hours\")).cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Keep only relevant columns\n",
    "cols_keep = [\n",
    "    # Metadata\n",
    "    \"service_type\", \"month\", \"pickup_ts\", \"dropoff_ts\",\n",
    "    \"pickup_hour\", \"pickup_dow\", \"is_weekend\",\n",
    "    # Location IDs\n",
    "    \"PULocationID\", \"DOLocationID\",\n",
    "    # engineered trip metrics\n",
    "    \"distance_mi\", \"trip_time_s\", \"mph\", \"active_hours\",\n",
    "    # Feature engineering\n",
    "    \"revenue\", \"expenses_nonfuel\", \"expense_fuel\",\n",
    "    \"net_before_fuel\", \"net_after_fuel\",\n",
    "    \"net_per_hr_before_fuel\", \"net_per_hr_after_fuel\",\n",
    "    # Parameters\n",
    "    \"price_per_gallon\", \"price_usd_per_kwh\",\n",
    "    \"energy_cost_per_mile\", \"ev_share\", \"mpg\", \"kwh_per_mile\",\n",
    "]\n",
    "\n",
    "# Filter columns to keep only those that exist in the DataFrame\n",
    "cols_keep = [c for c in cols_keep if c in df.columns]\n",
    "\n",
    "# Print size after cleaning\n",
    "df = df.select(cols_keep)\n",
    "print(f\"Cleaned data count: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17467e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results \n",
    "from pyspark.sql.functions import col, sum as ssum, count, round as sround, when, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Testing\n",
    "FAST_MODE = False  \n",
    "if FAST_MODE:\n",
    "    df_base = df.stat.sampleBy(\"service_type\", {\"yellow\": 0.02, \"hv_fhv\": 0.02}, seed=7)\n",
    "else:\n",
    "    df_base = df\n",
    "\n",
    "df_base = df.repartition(320, \"service_type\", \"month\")\n",
    "\n",
    "# Aggregate functions\n",
    "sum_net   = ssum(\"net_after_fuel\").alias(\"sum_net\")\n",
    "sum_hours = ssum(\"active_hours\").alias(\"sum_hours\")\n",
    "\n",
    "def safe_net_per_hr(df_agg):\n",
    "    return df_agg.withColumn(\n",
    "        \"net_per_hr_TW\",\n",
    "        sround(F.when(col(\"sum_hours\") > 0, col(\"sum_net\")/col(\"sum_hours\")), 2)\n",
    "    )\n",
    "\n",
    "# Overall net per hour\n",
    "overall = (\n",
    "    df_base.groupBy(\"service_type\")\n",
    "           .agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    ")\n",
    "overall = safe_net_per_hr(overall)\n",
    "\n",
    "\n",
    "# Hour of day\n",
    "hod = (\n",
    "    df_base.groupBy(\"service_type\", \"pickup_hour\")\n",
    "           .agg(sum_net, sum_hours, count(\"*\").alias(\"trips\"))\n",
    ")\n",
    "hod = safe_net_per_hr(hod).orderBy(\"service_type\", \"pickup_hour\")\n",
    "\n",
    "\n",
    "# Earnings per trip\n",
    "comp = (\n",
    "    df_base.groupBy(\"service_type\")\n",
    "           .agg(\n",
    "               ssum(\"revenue\").alias(\"sum_rev\"),\n",
    "               ssum(\"expenses_nonfuel\").alias(\"sum_nonfuel\"),\n",
    "               ssum(\"expense_fuel\").alias(\"sum_fuel\"),\n",
    "               count(\"*\").alias(\"n_trips\")\n",
    "           )\n",
    "           .withColumn(\"rev_per_trip\",     sround(col(\"sum_rev\")/col(\"n_trips\"), 2))\n",
    "           .withColumn(\"nonfuel_per_trip\", sround(col(\"sum_nonfuel\")/col(\"n_trips\"), 2))\n",
    "           .withColumn(\"fuel_per_trip\",    sround(col(\"sum_fuel\")/col(\"n_trips\"), 2))\n",
    "           .select(\"service_type\",\"n_trips\",\"rev_per_trip\",\"nonfuel_per_trip\",\"fuel_per_trip\")\n",
    ")\n",
    "\n",
    "\n",
    "# Pickup density map\n",
    "zone_trips = (\n",
    "    df_base.groupBy(\"service_type\",\"PULocationID\")\n",
    "           .agg(count(\"*\").alias(\"trips\"))\n",
    "           .orderBy(\"service_type\", col(\"trips\").desc())\n",
    ")\n",
    "\n",
    "zone_pivot = (\n",
    "    df_base.groupBy(\"PULocationID\",\"service_type\").count()\n",
    "           .groupBy(\"PULocationID\")\n",
    "           .pivot(\"service_type\", [\"yellow\",\"hv_fhv\"])\n",
    "           .agg(F.first(\"count\"))\n",
    "           .fillna(0)\n",
    "           .withColumn(\"diff_pickups\", (col(\"yellow\") - col(\"hv_fhv\")).cast(\"int\"))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07644c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL (time-weighted net/hr):\n",
      "  service_type       sum_net     sum_hours      trips  net_per_hr_TW\n",
      "0       hv_fhv  2.083564e+09  3.641863e+07  115610585          57.21\n",
      "1       yellow  3.807237e+08  4.614187e+06   17193243          82.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Plotting results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import os\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "os.makedirs(\"plots/img\", exist_ok=True)\n",
    "\n",
    "# Overall Table\n",
    "overall_pd = overall.toPandas()\n",
    "print(\"\\nOVERALL (time-weighted net/hr):\")\n",
    "print(overall_pd)\n",
    "\n",
    "# Hour-of-day Plot\n",
    "hod_pd = hod.toPandas()\n",
    "hod_p = (\n",
    "    hod_pd.pivot(index=\"pickup_hour\", columns=\"service_type\", values=\"net_per_hr_TW\")\n",
    "          .reindex(range(24))  \n",
    "          .sort_index()\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4)) \n",
    "hod_p.plot(ax=ax, marker=\"o\", linewidth=1.8, title=\"Time-weighted Net Profit by Hour of Day\")\n",
    "\n",
    "ax.set_xlabel(\"Pickup time\")\n",
    "ax.set_ylabel(\"Net $ per active hour\")\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f\"${v:,.0f}\"))\n",
    "\n",
    "major_xticks = list(range(0, 24, 2))\n",
    "ax.set_xticks(major_xticks)\n",
    "ax.set_xticklabels([f\"{h:02d}:00\" for h in major_xticks], rotation=0)\n",
    "\n",
    "ax.set_xticks(range(24), minor=True)                  \n",
    "ax.grid(True, axis=\"y\", alpha=0.3)                    \n",
    "ax.grid(True, which=\"minor\", axis=\"x\", alpha=0.12)    \n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.axhline(0, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/hour_of_day.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Composition\n",
    "comp_pd = comp.toPandas().set_index(\"service_type\")[[\"rev_per_trip\",\"nonfuel_per_trip\",\"fuel_per_trip\"]]\n",
    "\n",
    "ax = comp_pd.plot(kind=\"bar\", stacked=True, title=\"Per-trip Revenue vs Costs\")\n",
    "ax.set_xlabel(\"Service type\")              \n",
    "ax.set_ylabel(\"USD per trip\")              \n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f\"${v:,.0f}\")) \n",
    "\n",
    "ax.set_ylim(bottom=0)      \n",
    "ax.axhline(0, linewidth=1) \n",
    "ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/composition.png\", dpi=150)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e12dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# GEOSPATIAL\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "# Load NYC Taxi Zones shapefile\n",
    "TAXI_ZONES_PATH = \"data/taxi_zones/taxi_zones.shp\"\n",
    "zones = gpd.read_file(TAXI_ZONES_PATH)[[\"LocationID\",\"geometry\",\"zone\",\"borough\"]]\n",
    "zones[\"LocationID\"] = pd.to_numeric(zones[\"LocationID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Difference between yellow and hv_fhv pickups\n",
    "zone_diff_pd = zone_pivot.toPandas().rename(\n",
    "    columns={\"PULocationID\": \"LocationID\", \"diff_pickups\": \"pickup_diff\"}\n",
    ")\n",
    "zone_diff_pd[\"LocationID\"] = pd.to_numeric(zone_diff_pd[\"LocationID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "zone_diff_pd[\"pickup_diff\"] = pd.to_numeric(zone_diff_pd[\"pickup_diff\"], errors=\"coerce\")\n",
    "\n",
    "z_diff = zones.merge(zone_diff_pd[[\"LocationID\",\"pickup_diff\"]], on=\"LocationID\", how=\"left\")\n",
    "\n",
    "vals = z_diff[\"pickup_diff\"].astype(float).to_numpy()\n",
    "vmax = float(pd.Series(vals).abs().max()) if len(vals) else 1.0\n",
    "norm = TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "z_diff.plot(\n",
    "    column=\"pickup_diff\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    legend=True,\n",
    "    norm=norm,\n",
    "    linewidth=0.2,\n",
    "    edgecolor=\"black\",\n",
    "    missing_kwds={\"color\": \"lightgrey\"},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Pickup Difference (Yellow − HVFHV)\")\n",
    "ax.set_axis_off()  \n",
    "\n",
    "cax = fig.axes[-1]\n",
    "cax.set_ylabel(\"Pickups (difference)\")\n",
    "cax.yaxis.set_major_formatter(mtick.StrMethodFormatter(\"{x:,.0f}\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/map_pickups_diff.png\", dpi=150)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff7bd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 21:00:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/08/25 21:27:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "[Stage 573:======================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=28.25  MAE=14.74  R²=0.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Machine learning model 1\n",
    "\n",
    "# Regression model to predict net profit per hour\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"PULocationID\",\"pickup_hour\",\"is_weekend\",\"month\",\"price_per_gallon\",\"price_usd_per_kwh\"\n",
    "]\n",
    "target = \"net_per_hr_after_fuel\"\n",
    "\n",
    "# Prepare data for ML\n",
    "df_ml = df.select(\"service_type\", *features, target).dropna()\n",
    "df_ml = df_ml.withColumn(\"is_weekend\", col(\"is_weekend\").cast(\"int\"))\n",
    "\n",
    "# Train-test split on june data\n",
    "train = df_ml.filter(col(\"month\") < \"2024-06\")\n",
    "test  = df_ml.filter(col(\"month\") == \"2024-06\")\n",
    "\n",
    "# Hot encoding\n",
    "idx_service = StringIndexer(inputCol=\"service_type\", outputCol=\"service_type_idx\", handleInvalid=\"keep\")\n",
    "idx_month = StringIndexer(inputCol=\"month\", outputCol=\"month_idx\", handleInvalid=\"keep\")\n",
    "idx_location = StringIndexer(inputCol=\"PULocationID\", outputCol=\"PULocationID_idx\", handleInvalid=\"keep\")\n",
    "ohe = OneHotEncoder(\n",
    "    inputCols=[\"service_type_idx\", \"month_idx\", \"PULocationID_idx\"],\n",
    "    outputCols=[\"service_type_ohe\", \"month_ohe\", \"PULocationID_ohe\"],\n",
    "    handleInvalid=\"keep\", dropLast=True\n",
    ")\n",
    "\n",
    "# Assembler for features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"pickup_hour\", \"is_weekend\", \"price_per_gallon\", \"price_usd_per_kwh\",\n",
    "               \"service_type_ohe\", \"month_ohe\", \"PULocationID_ohe\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "# Linear regression model\n",
    "linear = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target,\n",
    "    maxIter=200,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8,\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[idx_service, idx_month, idx_location, ohe, assembler, linear])\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "e_rmse = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "e_mae = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "e_r2 = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "print(f\"RMSE={e_rmse.evaluate(predictions):.2f}  MAE={e_mae.evaluate(predictions):.2f}  R²={e_r2.evaluate(predictions):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2080c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ML Plot 1\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "avg_pred = (predictions.groupBy(\"PULocationID\",\"service_type\")\n",
    "            .agg(F.avg(\"prediction\").alias(\"avg_pred\")))\n",
    "\n",
    "wide = (avg_pred.groupBy(\"PULocationID\")\n",
    "        .pivot(\"service_type\")\n",
    "        .agg(F.first(\"avg_pred\")))\n",
    "\n",
    "# Adjust service_type names to match yours\n",
    "svc_cols = [c for c in wide.columns if c != \"PULocationID\"]\n",
    "yellow_col = next((c for c in svc_cols if \"yellow\" in c.lower()), None)\n",
    "ride_col   = next((c for c in svc_cols if \"hv\" in c.lower() or \"fhv\" in c.lower()), None)\n",
    "\n",
    "pred_diff = (wide.withColumn(\"profit_diff\", F.col(ride_col) - F.col(yellow_col))\n",
    "             .select(\"PULocationID\",\"profit_diff\")\n",
    "             .toPandas()\n",
    "             .rename(columns={\"PULocationID\":\"LocationID\"}))\n",
    "\n",
    "pred_diff[\"LocationID\"] = pd.to_numeric(pred_diff[\"LocationID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Merge predictions with zone shapefile\n",
    "z_diff = zones.merge(pred_diff, on=\"LocationID\", how=\"left\")\n",
    "\n",
    "# --- Plot ---\n",
    "vals = z_diff[\"profit_diff\"].astype(float).to_numpy()\n",
    "vmax = float(pd.Series(vals).abs().max()) if len(vals) else 1.0\n",
    "norm = TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "z_diff.plot(\n",
    "    column=\"profit_diff\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    legend=True,\n",
    "    norm=norm,\n",
    "    linewidth=0.2,\n",
    "    edgecolor=\"black\",\n",
    "    missing_kwds={\"color\": \"lightgrey\"},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Predicted Net Profit Difference ($/hr, HVFHV − Yellow)\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "cax = fig.axes[-1]\n",
    "cax.set_ylabel(\"Δ $/hr (HVFHV − Yellow)\")\n",
    "cax.yaxis.set_major_formatter(mtick.StrMethodFormatter(\"{x:,.2f}\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/map_profit_diff.png\", dpi=150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c7f72a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/26 09:26:37 WARN Instrumentation: [8247b24a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/08/26 09:28:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/08/26 09:28:20 WARN Instrumentation: [8247b24a] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/08/26 10:13:09 WARN Instrumentation: [a2d62443] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/08/26 10:15:40 WARN Instrumentation: [a2d62443] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "[Stage 1008:=====================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OAXACA] Observed gap (Yellow − HVFHV): 31.84\n",
      "[OAXACA] Explained by mix:              2.74\n",
      "[OAXACA] Unexplained (service effect):  30.21\n",
      "[OAXACA] % mix:   8.6%   |   % unexplained:  94.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Machine learning model 2\n",
    "\n",
    "from pyspark.sql.functions import avg as Favg\n",
    "from pyspark.ml.stat import Summarizer\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Build a feature vector for the second model\n",
    "assembler_2 = VectorAssembler(\n",
    "    inputCols=[\"month_ohe\", \"PULocationID_ohe\", \"pickup_hour\", \"is_weekend\", \"price_per_gallon\", \"price_usd_per_kwh\"],\n",
    "    outputCol=\"features_2\"\n",
    ")\n",
    "\n",
    "# Reuse the fitted pipeline\n",
    "train_pp = model.transform(train)\n",
    "test_pp = model.transform(test)\n",
    "\n",
    "train2 = assembler_2.transform(train_pp).select(\"service_type\", \"features_2\", target)\n",
    "test2 = assembler_2.transform(test_pp).select(\"service_type\", \"features_2\", target)\n",
    "\n",
    "# OLS regression model\n",
    "ols = LinearRegression(\n",
    "    featuresCol=\"features_2\",\n",
    "    labelCol=target,\n",
    "    maxIter=200,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.8,\n",
    ")\n",
    "\n",
    "m_y = ols.fit(train2.filter(col(\"service_type\")==\"yellow\"))\n",
    "m_h = ols.fit(train2.filter(col(\"service_type\")==\"hv_fhv\"))\n",
    "\n",
    "βy, b0y = np.array(m_y.coefficients.toArray()), float(m_y.intercept)\n",
    "βh, b0h = np.array(m_h.coefficients.toArray()), float(m_h.intercept)\n",
    "\n",
    "# Feature importance\n",
    "mean_y = np.array(test2.filter(col(\"service_type\")==\"yellow\").select(Summarizer.mean(col(\"features_2\")).alias(\"m\")).first()[0])\n",
    "mean_h = np.array(test2.filter(col(\"service_type\")==\"hv_fhv\").select(Summarizer.mean(col(\"features_2\")).alias(\"m\")).first()[0])\n",
    "                                                      \n",
    "# Observed gap\n",
    "ybarY = float(test.filter(col(\"service_type\")==\"yellow\").select(Favg(target)).first()[0])\n",
    "ybarH = float(test.filter(col(\"service_type\")==\"hv_fhv\").select(Favg(target)).first()[0])\n",
    "gap   = ybarY - ybarH\n",
    "\n",
    "# Two-way decomposition\n",
    "mix = float(np.dot((mean_y - mean_h), βh))\n",
    "unexp = float(np.dot(mean_y, (βy - βh)) + (b0y - b0h))\n",
    "\n",
    "print(f\"[OAXACA] Observed gap (Yellow − HVFHV): {gap:.2f}\")\n",
    "print(f\"[OAXACA] Explained by mix:              {mix:.2f}\")\n",
    "print(f\"[OAXACA] Unexplained (service effect):  {unexp:.2f}\")\n",
    "if abs(gap) > 1e-12:\n",
    "    print(f\"[OAXACA] % mix: {100*mix/gap:5.1f}%   |   % unexplained: {100*unexp/gap:5.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50419a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "os.makedirs(\"plots/img\", exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "# bars\n",
    "ax.bar(0, gap, color=\"#bdbdbd\", label=\"Observed gap\")\n",
    "ax.bar(1, mix,   color=\"#1f77b4\", label=\"Explained by mix\")\n",
    "ax.bar(1, unexp, bottom=mix, color=\"#ff7f0e\", label=\"Unexplained (service effect)\")\n",
    "\n",
    "# axis & title\n",
    "ax.axhline(0, color=\"black\", lw=0.8, alpha=0.7)\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels([\"Observed\", \"Decomposition\"])\n",
    "ax.set_ylabel(\"USD per active hour\")\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f\"${v:,.0f}\"))\n",
    "ax.set_title(\"Yellow − HVFHV Net Earnings Gap (Oaxaca–Blinder)\")\n",
    "\n",
    "# annotations\n",
    "ax.text(0, gap*0.5, f\"${gap:,.2f}\", ha=\"center\", va=\"center\",\n",
    "        fontsize=12, color=\"white\", fontweight=\"bold\")\n",
    "\n",
    "if abs(gap) > 1e-12:\n",
    "    ax.text(1, mix*0.5, f\"{(mix/gap)*100:,.1f}%\\n(${mix:,.2f})\",\n",
    "            ha=\"center\", va=\"center\", color=\"white\", fontsize=11)\n",
    "    ax.text(1, mix + unexp*0.5, f\"{(unexp/gap)*100:,.1f}%\\n(${unexp:,.2f})\",\n",
    "            ha=\"center\", va=\"center\", color=\"white\", fontsize=11)\n",
    "\n",
    "ax.legend(frameon=False, loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/img/model2_oaxaca_bar_v2.png\", dpi=200)\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast30034-venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
